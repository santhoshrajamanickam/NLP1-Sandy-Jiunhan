{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Text Corpora and Language Modelling\n",
    "\n",
    "This lab is meant to help you get familiar with some language data, and use this data to estimate N-gram language models\n",
    "\n",
    "First you will use the **Penn Treebank**, which is a collection of newspaper articles from the newspaper \n",
    "The Wall Street Journal. The idea is to examine the data and notice interesting properties. This will not take more than a few lines of code.\n",
    "\n",
    "Then you will use a corpus consisting of **TedX** talks. This you will use to estimate an **N-gram language model** for different orders of N, and use this this for some tasks.\n",
    "\n",
    "The datasets are on blackboard under course materials. Download the zip and make sure to put the files in the same directory as the notebook.\n",
    "\n",
    "### Rules\n",
    "* The lab exercises should be made in **groups of two people**.\n",
    "\n",
    "* The deadline is **Tuesday 7 nov 16:59**.\n",
    "\n",
    "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**.\n",
    "\n",
    "* The **filename** should be `lab1_lastname1_lastname2.ipynb`, so for example `lab1_Jurafsky_Martin.ipynb`.\n",
    "\n",
    "* The notebook is graded on a scale of **0-10**. The number of points for each question is indicated in parantheses. \n",
    "\n",
    "* The questions marked **optional** are not graded; they are an additional challenge for those interested in going the extra mile. \n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should **write your code and answers in this iPython Notebook** (see http://ipython.org/notebook.html for reference material). If you have problems, please contact your teaching assistant.\n",
    "\n",
    "* Use only **one cell for code** and **one cell for markdown** answers!    \n",
    "\n",
    "    * Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
    "    \n",
    "    * For theoretical question, put your solution in the YOUR ANSWER HERE cell.\n",
    "    \n",
    "* Test your code and **make sure we can run your notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Penn treebank\n",
    "\n",
    "## Exercise 1.1 (40 points, 5 points per subquestion )\n",
    "\n",
    "You are provided with a corpus containing words with their Part-of-Speech tags (POS-tags for short). The format is\n",
    "**word|POS** (one sentence per line) and the file name is **sec02-22.gold.tagged**. This data is extracted from Sections 02-22 from the Penn Treebank: these sections are most commonly used for training statistical models like POS-taggers and parsers.\n",
    "\n",
    "**[Hint]** **Figure 10.1** in chapter 10 of Jurafsky and Martin (see [here](https://web.stanford.edu/~jurafsky/slp3/10.pdf)) holds a summary of the 45 POS-tags used in the Penn Treebank tagset together with their meaning and some examples. (If you are keen on learning more about the word-classes represented POS-tags and their definitions you can do a litle reading ahead for next week and already have a look at section 10.1 of the same chapter).\n",
    "\n",
    "**[Hint]** the Python library [collections](https://docs.python.org/2/library/collections.html) has an object called `Counter` which will come in handy for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** How large is the corpus? (i.e. how **many tokens**). And what is the size of the **vocabulary** used in this corpus? \n",
    "\n",
    "Estimate the vocabulary size both by **lowercasing** all the words as well as by leaving the words in their **original orthography**. What is an advantage of lowercasing all the words in your corpus? What is a notable downside? **Give examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 929552\n",
      "Vocabulary size of uppercase: 44210\n",
      "Vocabulary size of lowercase: 39384\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "context = open('sec02-21.gold.tagged').read()\n",
    "lower_context = open('sec02-21.gold.tagged').read().lower()\n",
    "\n",
    "\n",
    "#TODO remove \\n by regular expression?\n",
    "lower_context = lower_context.replace('\\n', '')\n",
    "lower_words = re.findall(r'(?:\\ |^)([^ ]+)(?:\\|)', lower_context)\n",
    "\n",
    "context = context.replace('\\n', '')\n",
    "words = re.findall(r'(?:\\ |^)([^ ]+)(?:\\|)', context)\n",
    "#print(Counter(words).values())\n",
    "c = Counter({'a':1, 'b':2})\n",
    "print(\"Number of tokens:\", sum(Counter(words).values()))\n",
    "print(\"Vocabulary size of uppercase:\", len(Counter(words)))\n",
    "print(\"Vocabulary size of lowercase:\", len(Counter(lower_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "By lowercasing all the words, we can reduce our data size. But words have different meaning between they are at start position(uppercase) and in the middle of sentence(lowercase). Take 'what' as example, when 'what' at the begining of sentence, it could be used for creating an interrogative sentence. But when 'what' is in the middle of sentence, 'what' represents as relative pronoun. If we lowercase all words in the corpus, we will eliminate  the dependency between the meaning of words and the position of words in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "For the rest of this exercise you should use the **original orthography** of the data when answering the questions.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Plot a graph of word frequency versus rank of a word, in this corpus. Does this corpus obey **Zipfâ€™s law**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6828b99208>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIkEpIEREJCgFhIiIBPX/e+axj75bE5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f682eab1908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_counter = Counter(words).most_common(10000)\n",
    "y = [int(frequency[1]) for frequency in y_counter]\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Examination of Zipf\\'s law')\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** What are the **20 most common words** in the corpus and how often do they occur? What is the 50th most common word, the 100th and the 1000th and how often do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top20:\n",
      "(',', 48310)\n",
      "('the', 40839)\n",
      "('.', 38798)\n",
      "('of', 22782)\n",
      "('to', 22056)\n",
      "('a', 19171)\n",
      "('and', 15906)\n",
      "('in', 15085)\n",
      "(\"'s\", 9249)\n",
      "('that', 7951)\n",
      "('for', 7912)\n",
      "('$', 7125)\n",
      "('is', 6893)\n",
      "('The', 6791)\n",
      "('said', 5597)\n",
      "('on', 5112)\n",
      "('%', 4871)\n",
      "('it', 4639)\n",
      "('by', 4450)\n",
      "('from', 4437)\n",
      "50th: ('had', 1755)\n",
      "100th: ('A', 860)\n",
      "1000th: ('subject', 109)\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "top20 = Counter(words).most_common(20)\n",
    "print(\"Top20:\")\n",
    "for count_word in top20:\n",
    "    print(count_word)\n",
    "print(\"50th:\", Counter(words).most_common(50)[49])\n",
    "print(\"100th:\", Counter(words).most_common(100)[99])\n",
    "print(\"1000th:\", Counter(words).most_common(1000)[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** How many different Part-of-speech tags are present in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 different part-of-speech tags\n"
     ]
    }
   ],
   "source": [
    "pos = re.findall(r'(?:\\|)([^ ]+)(?:\\ )', context)\n",
    "print(len(Counter(pos)), \"different part-of-speech tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Print a list of the **10 most commonly occurring POS tags** in the data. For each of these POS tags, what are the **3 most common words** that belong to that class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top10:\n",
      "('NN', 132134)\n",
      "('IN', 99413)\n",
      "('NNP', 90711)\n",
      "('DT', 82147)\n",
      "('JJ', 59643)\n",
      "('NNS', 59332)\n",
      "(',', 48314)\n",
      "('.', 39252)\n",
      "('CD', 36148)\n",
      "('RB', 30232)\n",
      "-------------------------------\n",
      "NN: \n",
      "% 4866\n",
      "Mr. 4147\n",
      "company 2457\n",
      "-------------------------------\n",
      "IN: \n",
      "of 22778\n",
      "in 14852\n",
      "for 7907\n",
      "-------------------------------\n",
      "NNP: \n",
      "Mr. 4147\n",
      "U.S. 1577\n",
      "Corp. 1186\n",
      "-------------------------------\n",
      "DT: \n",
      "the 40831\n",
      "a 19151\n",
      "The 6753\n",
      "-------------------------------\n",
      "JJ: \n",
      "new 1396\n",
      "other 1298\n",
      "more 1117\n",
      "-------------------------------\n",
      "NNS: \n",
      "years 1164\n",
      "shares 1128\n",
      "sales 939\n",
      "-------------------------------\n",
      ",: \n",
      ", 48310\n",
      "underwriters 1\n",
      "an 1\n",
      "-------------------------------\n",
      ".: \n",
      ". 38798\n",
      "? 392\n",
      "! 62\n",
      "-------------------------------\n",
      "CD: \n",
      "million 4355\n",
      "billion 1780\n",
      "one 1203\n",
      "-------------------------------\n",
      "RB: \n",
      "n't 3211\n",
      "also 1420\n",
      "not 1288\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "all_pos = re.findall(r'(?:\\|)([^ ]+)(?:\\ )', context)\n",
    "top10 = Counter(all_pos).most_common(10)\n",
    "print(\"Top10:\")\n",
    "for count_pos in top10:\n",
    "    print(count_pos)\n",
    "\n",
    "for pos in top10:\n",
    "    pos_in_re = pos[0]\n",
    "    word_and_pos = re.findall(r'(?: |^)([^ ]+\\|' + re.escape(pos[0]) + ')', context)\n",
    "    print('-------------------------------')    \n",
    "    print(pos[0] + \": \")\n",
    "    for top3 in Counter(word_and_pos).most_common(3):        \n",
    "        print(top3[0].split('|')[0], top3[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** A single word may have several POS-tags. For example, *record* can be a both a **noun** *(buy a record)* or a **verb** *(record a lecture)*. This make POS-tags extremely useful for **disambiguation**.\n",
    "\n",
    "What percentage of the words in the vocabulary is **ambiguous**? (i.e. have more than one POS tag?) What are the 10 most frequent combinations of POS tags in the case of ambitguity? Which words are **most ambiguous**? Give some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.47%\n",
      "\n",
      "Top 10 most frequent combination:\n",
      "['VBD', 'VBN']\n",
      "['JJ', 'NN']\n",
      "['NN', 'NNP']\n",
      "['VB', 'VBP']\n",
      "['NNS', 'VBZ']\n",
      "['NN', 'VB']\n",
      "['NN', 'VBG']\n",
      "['JJ', 'VBD', 'VBN']\n",
      "['NN', 'VB', 'VBP']\n",
      "['JJ', 'NNP']\n",
      "\n",
      "Most ambiguous words:\n",
      "('set', 7)\n",
      "('open', 7)\n",
      "('down', 7)\n",
      "('many', 7)\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "word_and_all_pos = re.findall(r'(?: |^)([^ ]+\\|[^ ]+)', context)\n",
    "word_pos_count = {}\n",
    "for word in word_and_all_pos:\n",
    "    word_split = word.split('|')\n",
    "    \n",
    "    if word_split[0] not in word_pos_count:\n",
    "        word_pos_count[word_split[0]] = {}\n",
    "        if word_split[1] not in word_pos_count[word_split[0]]:\n",
    "            word_pos_count[word_split[0]][word_split[1]] = 1\n",
    "        else:\n",
    "            word_pos_count[word_split[0]][word_split[1]] += 1\n",
    "    else:\n",
    "        if word_split[1] not in word_pos_count[word_split[0]]:\n",
    "            word_pos_count[word_split[0]][word_split[1]] = 1\n",
    "        else:\n",
    "            word_pos_count[word_split[0]][word_split[1]] += 1\n",
    "            \n",
    "count_ambiguity = 0\n",
    "num_of_words = len(word_pos_count)\n",
    "ambiguity_values = {}\n",
    "pos_combinations = {}\n",
    "\n",
    "for word, pos_dict in word_pos_count.items():\n",
    "    \n",
    "    if len(word_pos_count[word]) >= 2:\n",
    "        count_ambiguity += 1\n",
    "        ambiguity_values[word] = len(word_pos_count[word])    \n",
    "        #ambiguous_words.append(word)\n",
    "        \n",
    "        pos_combination = ''\n",
    "        sorted_pos_combination = sorted(pos_dict.items(), key=operator.itemgetter(0))\n",
    "        for pos in sorted_pos_combination:            \n",
    "            pos_combination += pos[0] + '_SEPERATOR_'           \n",
    "\n",
    "        if pos_combination not in pos_combinations:\n",
    "            pos_combinations[pos_combination] = 1\n",
    "        else:\n",
    "            pos_combinations[pos_combination] += 1\n",
    "    \n",
    "sorted_ambiguity_values = sorted(ambiguity_values.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_pos_combinations = sorted(pos_combinations.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "top4_ambiguous_words = sorted_ambiguity_values[:4]\n",
    "top10_pos_combinations = sorted_pos_combinations[:10]\n",
    "\n",
    "percentage_ambiguity = count_ambiguity / num_of_words\n",
    "\n",
    "print(str(float(\"{:.2f}\".format(percentage_ambiguity * 100))) + '%')\n",
    "\n",
    "print('\\nTop 10 most frequent combination:')\n",
    "\n",
    "for top10_pos_combination in top10_pos_combinations:\n",
    "    top10_pos_combination_split = top10_pos_combination[0].split('_SEPERATOR_')\n",
    "    del top10_pos_combination_split[-1]\n",
    "    print(top10_pos_combination_split)\n",
    "\n",
    "print('\\nMost ambiguous words:')\n",
    "    \n",
    "for top4_ambiguous_word in top4_ambiguous_words:\n",
    "    print(top4_ambiguous_word)\n",
    "\n",
    "\n",
    "    \n",
    "#print(Counter(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **(g)**  Print some of these words with their multiple POS-tags. Do you **understand the ambiguity**? Use figure 10.1 mentioned above to interpret the POS-tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade\n",
      "VBZ, VBP, NN, VB\n",
      "\n",
      "Well\n",
      "RB, UH, NNP\n",
      "\n",
      "Young\n",
      "NNP, NN, JJ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "count = 0\n",
    "count_pick = 0\n",
    "for word, pos_dict in word_pos_count.items():\n",
    "    count_pick += 1\n",
    "    if count >= 3:\n",
    "        break\n",
    "        \n",
    "    if len(word_pos_count[word]) >= 3 and count_pick % 5 == 3:\n",
    "        count += 1\n",
    "        print(word)\n",
    "        pos_string = ''\n",
    "        for pos in pos_dict:\n",
    "            pos_string += pos + ', '\n",
    "        pos_string = pos_string[0:-2]\n",
    "        print(pos_string + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reserve: \n",
    "VBP: \n",
    "NN: reserve price\n",
    "VB: \n",
    "\n",
    "audited:\n",
    "VBN: \n",
    "VBD: \n",
    "JJ:\n",
    "\n",
    "compound:\n",
    "NN: \n",
    "VB: \n",
    "JJ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Ambiguous words do not account for a great percentage of the vocabulary. Yet they are among the most commonly occuring words of the English language. What **percentage of the dataset is ambiguous**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.08%\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "num_ambiguous_words = 0\n",
    "for word in words:\n",
    "    if word in ambiguity_values:\n",
    "        num_ambiguous_words += 1\n",
    "percentage_ambiguous_data = num_ambiguous_words / len(words)\n",
    "print(str(float(\"{:.2f}\".format(percentage_ambiguous_data * 100))) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 (10 points, 5 per subquestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also provided with another file called **sec00.gold.tagged**. \n",
    "Section 00 of the Penn Treebank is typically used as development data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** How many **unseen words** are present in the development data (i.e., words that have not occurred in the training data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288 words are unseen words\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "dev_context = open('sec00.gold.tagged').read()\n",
    "dev_context = dev_context.replace('\\n', '')\n",
    "dev_words = re.findall(r'(?:\\ |^)([^ ]+)(?:\\|)', dev_context)\n",
    "\n",
    "#print(words[10])\n",
    "words_dict = {}\n",
    "for word in words:\n",
    "    words_dict[word] = 1\n",
    "dev_words = list(set(dev_words))\n",
    "unseen_words = {}\n",
    "for dev_word in dev_words:\n",
    "    if dev_word not in words_dict:\n",
    "        unseen_words[dev_word] = 1\n",
    "        \n",
    "print(len(unseen_words), 'words are unseen words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** What are the three **POS tag categories** that the most **unseen words** belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "JJ\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "word_and_all_pos = re.findall(r'(?: |^)([^ ]+\\|[^ ]+)', dev_context)\n",
    "word_pos_count = {}\n",
    "for word in word_and_all_pos:\n",
    "    word_split = word.split('|')\n",
    "    \n",
    "    if word_split[0] not in word_pos_count:\n",
    "        word_pos_count[word_split[0]] = {}\n",
    "        if word_split[1] not in word_pos_count[word_split[0]]:\n",
    "            word_pos_count[word_split[0]][word_split[1]] = 1\n",
    "        else:\n",
    "            word_pos_count[word_split[0]][word_split[1]] += 1\n",
    "    else:\n",
    "        if word_split[1] not in word_pos_count[word_split[0]]:\n",
    "            word_pos_count[word_split[0]][word_split[1]] = 1\n",
    "        else:\n",
    "            word_pos_count[word_split[0]][word_split[1]] += 1\n",
    "\n",
    "pos_count = {}\n",
    "            \n",
    "for word, pos_dict in word_pos_count.items():\n",
    "    if word in unseen_words:\n",
    "        for pos in pos_dict:\n",
    "            if pos not in pos_count:\n",
    "                pos_count[pos] = 1\n",
    "            else:\n",
    "                pos_count[pos] += 1\n",
    "\n",
    "pos_count = sorted(pos_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for i in range(3):\n",
    "    print(pos_count[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language Models\n",
    "\n",
    "This part of the lab will be covered in the Wednesday lecture. If you have prior exposure to NLP, go ahead and finish this part! If you don't, start anyway, and this part will be clear after the lecture. \n",
    "\n",
    "Reference **chapter 4** of J&M *Language Modeling with N-Grams*. \n",
    "\n",
    "----------\n",
    "\n",
    "Models that assign **probabilities** to **sequences of words** are called language **language\n",
    "modelels** or **LMs**. The simplest model that assigns probabilities to sentences and sequences of words is the **N-gram** model.\n",
    "\n",
    "Recall that an *N*-gram language model uses **conditional probabilities** of the form\n",
    "    \n",
    "$$P(w_k \\mid w_{k-N+1} \\dots w_{k-1})$$\n",
    "\n",
    "to **approximate** the full **joint probability**\n",
    "\n",
    "$$P(w_1 \\dots w_n)$$\n",
    "\n",
    "of a sequence of words $w_1 \\dots w_n$.\n",
    "\n",
    "The easiest way of obtaining estimates for the probabilities $P(w_k \\mid w_{k-N+1} \\dots w_{k-1})$ is to use the **maximum likelihood estimate** or **MLE**, a widely used statistical estimation method ([read more]((https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)). You count and normalize:\n",
    "\n",
    "$$P_{MLE}(w_k \\mid w_{k-N+1} \\dots w_{k-1}) = \\frac{C(w_{k-N+1} \\dots w_{k-1} w_k)}{C(w_{k-N+1} \\dots w_{k-1})}.$$\n",
    "\n",
    "\n",
    "## Exercise 2.1 (25 points)\n",
    "\n",
    "**(a) ** Complete the function `train_ngram` so that you can train a count-based $N$-gram language model on the data found in `data/ted-train.txt` and train this for $N=2,3,4$. **15 points**\n",
    "\n",
    "**(b) ** Extend the function above so that it accepts a parameter `k` for optional add-$k$ smoothing. **10 points**\n",
    "\n",
    "**[Datastructure hint]** If you store the smoothed language in a naive manner (that is, to store *all* the numbers separately) your datastructure will get huge! If $V$ is the vocabulary then the smoothed bigram model assigns probabilities to $|V|^2$ entries. If $|V|$ is around 80k, the naive way requires you to store more than 64 billion floats. Yet almost all of these are actually just $P(w_n|w_{n-1}) = \\frac{k}{N + k|V|}$, with $k$ the value with which you smooth and $N=C(w_{n-1})$. Think about how you use this fact to make your model work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Python hint]** The `collections` library has another useful datastructure: the `defaultdict`. Some example uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(float)\n",
    "d[\"new key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that to an ordinary dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = dict()\n",
    "#d[\"new key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other datatypes as `default_factory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(int)\n",
    "d[\"new key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(list)\n",
    "d[\"new key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting an already existing `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "d1 = {k: \"value\" for k in range(1, 11)}\n",
    "d = defaultdict(float, d1) # convert it to a defaultdict\n",
    "print(d[5])\n",
    "print(d[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#d = defaultdict(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `lambda` to make the number `10` `callable`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(lambda: 10)\n",
    "d[\"new key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(lambda: defaultdict(float))\n",
    "d[\"new key\"]['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clever use of a `defaultdict` can be the solution to the problem of data-storing in a smoothing $N$-gram pointed out above:\n",
    "    \n",
    "    ngram = defaultdict(lambda: k/(N+kV), ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"ted-train.txt\"\n",
    "\n",
    "def read(fname, max_lines=np.inf):\n",
    "    \"\"\"\n",
    "    Reads in the data in fname and returns it as\n",
    "    one long list of words. Also returns a vocabulary in\n",
    "    the form of a word2index and index2word dictionary.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # w2i will automatically keep a counter to asign to new words\n",
    "    w2i = defaultdict(lambda: len(w2i))\n",
    "    i2w = dict()\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    \n",
    "    with open(fname, \"r\") as fh:\n",
    "        for k, line in enumerate(fh):\n",
    "            if k > max_lines:\n",
    "                break\n",
    "            words = line.strip().split()\n",
    "            # assign an index to each word\n",
    "            for w in words:\n",
    "                i2w[w2i[w]] = w # trick\n",
    "            \n",
    "            sent = [start] + words + [end]\n",
    "            data.append(sent)\n",
    "\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ngram(data, N, k=0):\n",
    "    \"\"\"\n",
    "    Trains an n-gram language model with optional add-k smoothing\n",
    "    and additionaly returns the unigram model\n",
    "\n",
    "    :param data: text-data as returned by read\n",
    "    :param N: (N>1) the order of the ngram e.g. N=2 gives a bigram\n",
    "    :param k: optional add-k smoothing\n",
    "    :returns: ngram and unigram\n",
    "    \"\"\"\n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    data_copy = data.copy()    \n",
    "    ngram = defaultdict(Counter) # ngram[history][word] = #(history,word)\n",
    "    \n",
    "    if N - 2 > 0:\n",
    "        for sent_idx, sent in enumerate(data_copy):\n",
    "            for i in range(0, N-2):\n",
    "                data_copy[sent_idx] = [\"<s>\"] + data_copy[sent_idx]    \n",
    "    unpacked_data = [word for sent in data_copy for word in sent]\n",
    "    unigram = defaultdict(float, Counter(unpacked_data)) # default prob is 0.0        \n",
    "    \n",
    "    \n",
    "    V = len(unigram)\n",
    "    num_of_word_tokens = len(unpacked_data)\n",
    "    for i in range(N-1, len(unpacked_data)):   \n",
    "        #TODO remove this\n",
    "        #if i > 1000:\n",
    "        #    break\n",
    "        context = tuple(unpacked_data[i - N + 1:i])\n",
    "        ngram[context][unpacked_data[i]] += 1\n",
    "        \n",
    "    for context, count in ngram.items():\n",
    "        total = sum(count.values())\n",
    "        for word in count:\n",
    "            count[word] += k\n",
    "            count[word] /= float(total+k*V)\n",
    "        #count_sum = sum(count.values())\n",
    "        #for word in count:\n",
    "        #    count[word] /= count_sum +\n",
    "        ngram[context] = defaultdict(lambda total=total, k=k, V=V: k/float(total+k*V), ngram[context])\n",
    "                    \n",
    "    for context, count in unigram.items():\n",
    "        unigram[context] += k\n",
    "        unigram[context] /= float(num_of_word_tokens + k*V)\n",
    "        \n",
    "    unigram = defaultdict(lambda num_of_word_tokens=num_of_word_tokens, k=k, V=V: k/float(num_of_word_tokens+k*V), unigram)\n",
    "    return ngram, unigram\n",
    "\n",
    "data, w2i, i2w = read(train_file)\n",
    "bigram, unigram = train_ngram(data, N=2, k=0)\n",
    "bigram_smoothed, unigram_smoothed = train_ngram(data, N=2, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1803866946811776e-05\n",
      "0.0908425600226628\n",
      "785\n",
      "77020\n",
      "0.909133832243443\n",
      "0.9999763922661058\n"
     ]
    }
   ],
   "source": [
    "default_prob = bigram_smoothed[(\"They\", )][\"They\"] # \"all all\" does not occur => equals k / (N + kV)\n",
    "print(default_prob)\n",
    "d = bigram_smoothed[(\"They\", )]                    # the smoothed conditional distribution p(w|\"all\")\n",
    "#print(d)\n",
    "s = sum(d.values())                       # total probability mass actually stored in d (< 1.0)\n",
    "print(s)\n",
    "seen = len(d.keys())                      # number of actual words stored in d\n",
    "print(seen)\n",
    "unseen = len(w2i) - seen                  # all other words\n",
    "print(unseen)\n",
    "remaining_prob = default_prob * unseen    # all remaining probability mass not actually stored in d\n",
    "print(remaining_prob)\n",
    "one = s + remaining_prob                  # should equal 1.0\n",
    "print(one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 (5 points)\n",
    "\n",
    "You can use an *N*-gram language model to **generate text**. The higher the order *N* the better your model will be able to catch the long-range dependecies that occur in actual sentences and the better your changes are at generating sensible text. But beware: **sparsity** of language data will quickly cause your model to reproduce entire lines from your training data; in such cases only one $w_k$ was observed for the histories $w_{k-N+1}\\dots w_{k-1}$ in the entire training-set.\n",
    "\n",
    "**Complete** the function `generate_sent`. It takes a language model `lm` and an order `N` and should generate a sentence by **sampling** from the language model.\n",
    "\n",
    "**[Hint]** You can use the method of [inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling) to generate a sample from a **categorical distribution**, $p_1\\dots p_k$ such that $p_i \\geq 0$ and $\\sum_{i=1}^k p_i = 1$, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Counter({1: 501, 2: 208, 0: 200, 3: 91})\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "P = [0.2,0.5,0.2,0.1]\n",
    "\n",
    "def sample(P):\n",
    "    u = random() # uniformly random number between 0 and 1\n",
    "    p = 0\n",
    "    for i, p_i in enumerate(P):\n",
    "        #TODO p += p_i should be here?\n",
    "        p += p_i\n",
    "        if p > u: \n",
    "            return i # the first i s.t. p1 + ... + pi > u\n",
    "        #p += p_i\n",
    "\n",
    "print(sample(P))\n",
    "\n",
    "print(Counter([sample(P) for i in range(1000)])) # check to see if the law of large numbers is still true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform sampling in the words of Jurafsky and Martin:\n",
    "\n",
    "> Imagine all the words of the English language covering the probability space\n",
    "between 0 and 1, each word covering an interval proportional to its frequency. We\n",
    "choose a random value between 0 and 1 and print the word whose interval includes\n",
    "this chosen value.\n",
    "\n",
    "(J&M, section 4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another adaptation would be less tired and you apply the brakes computer , starts to weep . "
     ]
    }
   ],
   "source": [
    "#N_grams = 2\n",
    "#ngram, unigram = train_ngram(data, N_grams, 1)    \n",
    "#TODO what's lm???\n",
    "\n",
    "def generate_sent(lm, N):\n",
    "    #global N_grams, ngram\n",
    "    if len(lm) + 1 <= N - 1:\n",
    "        for n in range(len(lm) + 1, N - 1 + 1):\n",
    "            lm.append('<s>')             \n",
    "\n",
    "    '''for n in range(2, N):\n",
    "        print('n', n)\n",
    "        ngram, unigram = train_ngram(data, n, 1)\n",
    "        new_word = ''\n",
    "        #if n == 1:                \n",
    "        #    s = sample(list(unigram.values()))\n",
    "        #print(s)\n",
    "        #    new_word = list(unigram.keys())[s]\n",
    "        #else:\n",
    "        tuple_lm = tuple(lm)\n",
    "        s = sample(list(ngram[tuple_lm].values()))\n",
    "        #print(s)\n",
    "        new_word = list(ngram[tuple_lm].keys())[s]\n",
    "\n",
    "        lm.append(new_word)\n",
    "        del ngram\n",
    "    '''\n",
    "    #print(lm)\n",
    "        \n",
    "    ngram, unigram = train_ngram(data, N, 0)  \n",
    "    count = 0\n",
    "    #for i in range(N_grams - 1, N):\n",
    "    new_word = ''\n",
    "    while new_word != '</s>':\n",
    "        count += 1\n",
    "        if count > 1000:\n",
    "            break\n",
    "        tuple_list = []\n",
    "        #print(i, N_grams, i - N_grams) \n",
    "        start_j = len(lm) - 1 - N + 2\n",
    "        end_j = len(lm)\n",
    "        \n",
    "        if start_j < 0:\n",
    "            start_j = 0\n",
    "        if end_j < 0:\n",
    "            end_j = 0\n",
    "        #print('se', start_j, end_j)\n",
    "        for j in range(start_j, end_j):\n",
    "            #print('j', j)                        \n",
    "            tuple_list.append(lm[j])\n",
    "        tuple_lm = tuple(tuple_list)\n",
    "        #print(tuple_lm, ngram[tuple_lm])\n",
    "        #print(ngram[tuple_lm])\n",
    "        s = sample(list(ngram[tuple_lm].values()))\n",
    "        if s is None:\n",
    "            s = 0\n",
    "            new_word = list(ngram[tuple_lm].keys())[s]\n",
    "        else:\n",
    "            new_word = list(ngram[tuple_lm].keys())[s]\n",
    "        #print('hi', ngram[tuple_lm].keys())\n",
    "        #print(ngram[tuple_lm].values())\n",
    "        #print(sum(ngram[tuple_lm].values()))\n",
    "        #print(s)\n",
    "        '''if len(ngram[tuple_lm]) < 50:\n",
    "            print(ngram[tuple_lm].values())\n",
    "            print(sum(ngram[tuple_lm].values()))\n",
    "            print(s)'''\n",
    "        \n",
    "        \n",
    "        if new_word == '</s>':\n",
    "            break\n",
    "        lm.append(list(ngram[tuple_lm].keys())[s])\n",
    "        \n",
    "        \n",
    "    lm = lm[N - 1:]\n",
    "    return lm\n",
    "            \n",
    "    \n",
    "lm = generate_sent([], 3)\n",
    "for word in lm:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional]\n",
    "\n",
    "For how many of the histories $w_{k-N+1}\\dots w_{k-1}$ is the number of continuations $w_n$ equal to **one**? Calculate the percentage of such cases for the different orders *N*.\n",
    "\n",
    "And which history has the **most possible continuations**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ANSWER ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2.3 (5 points)\n",
    "\n",
    "Let $V$ denote our vocabulary. Recall that for any $w$ in $V$ `bigram[w]` defines a conditional probability $p(v|w)$ over $v$ in $V$. In the case of an **unsmoothed bigram**, $p(v|w) = 0$ for most $v\\in V$, whereas in the **smoothed bigram** smoothing took care that $p(v|w) \\geq 0$ for *all* $v$.\n",
    "\n",
    "The function `plot_bigram_dist(word, bigram, smoothbigram, k=30)` plots shows $p(v|word)$ for the `k` words $v$. One bar shows the probabilities in `bigram` and one in `smoothbigram`. \n",
    "\n",
    "**(a)** Use this function to plot the distribution for at least two words `w` and answer the questions\n",
    "* What is the effect of smoothing on the bigram distribution of frequent words? \n",
    "* What is the effect in the case of infrequent words?\n",
    "* Explain the difference between the two based on the raw counts of `w` \n",
    "\n",
    "**(b)** Now experiment with $k$ much smaller than 1 (but greater than 0!) \n",
    "* What are the effects?\n",
    "\n",
    "\n",
    "**[Hint]** Remember that add-1 smoothing turns \n",
    "$$P(w_n\\mid w_{n-1}) = \\frac{C(w_{n-1}w_{n})}{C(w_{n-1})}$$\n",
    "into\n",
    "$$P_{add-1}(w_n\\mid w_{n-1}) = \\frac{C(w_{n-1}w_{n}) + 1}{C(w_{n-1}) + |V|}.$$\n",
    "\n",
    "What happens when $C(w_{n-1})$ is relatively big (similiar in of size as $ |V| $)? And what if $C(w_{n-1})$ is small? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns    \n",
    "\n",
    "def plot_bigram_dist(word, bigram, smoothbigram, k=30):\n",
    "    d = bigram[word]\n",
    "    ds = smoothbigram[word]\n",
    "    \n",
    "    # sort the probabilities\n",
    "    d_sort = sorted(d.items(), reverse=True, key=lambda t: t[1])[0:k]\n",
    "    ds_sort = sorted(ds.items(), reverse=True, key=lambda t: t[1])[0:k]\n",
    "    \n",
    "    _, probs = zip(*d_sort)\n",
    "    smooth_ws, smooth_probs = zip(*ds_sort)\n",
    "    \n",
    "    # make up for the fact that in the unsmoothed case  probs is generally less than k long\n",
    "    probs = probs + (0,) * (k-len(probs)) \n",
    "\n",
    "    w_data = pd.DataFrame({\"w\": smooth_ws * 2,\n",
    "                           \"P({}|w)\".format(word): probs + smooth_probs,\n",
    "                           \"smoothing\": [\"unsmoothed\"]*k + [\"smoothed\"]*k})\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    plt.xticks(rotation=90)\n",
    "    g = sns.barplot(ax=ax, x=\"w\", y=\"P({}|w)\".format(word), hue=\"smoothing\",\n",
    "                    data=w_data, palette=\"Blues_d\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnEAAAJiCAYAAACoz5QNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xu03XV95//XOyGClJuDUatIExUL\nCAYhglRTlYyaegGx+FOso4ALfx2L7dhpFGZZsI7WS622tVVKB4R6KXEAKypVpMgAKkqIh5tojRh+\nBLAgioLDxeDn98feoceYyzmYnX0+5PFYKyt7f/d3f/M+2Wb55Hvb1VoLAAB9mTXuAQAAmD4RBwDQ\nIREHANAhEQcA0CERBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA0KFtxj3AlvDIRz6yzZs3b9xj\nAABs0hVXXPGD1trcTa23VUTcvHnzsnz58nGPAQCwSVV1w1TWczgVAKBDIg4AoEMiDgCgQ1vFOXEA\nwC/62c9+ltWrV+eee+4Z9yhbre222y677bZb5syZ86DeL+IAYCu0evXq7Ljjjpk3b16qatzjbHVa\na7n99tuzevXqzJ8//0Ftw+FUANgK3XPPPdl1110F3JhUVXbddddfaU+oiAOArZSAG69f9e9fxAEA\nXVq1alU+8YlPPPD89NNPz3HHHbfedV/4whfmjjvu2FKjbREiDgDo0roRtzHnnXdedtlllxFPtGWJ\nOABgs/rpT3+aF73oRVmwYEH22WefLFu2LPPmzcsJJ5yQ/fbbLwsXLsyKFSvyghe8IE984hNz8skn\nJxmc7L906dLss88+2XfffbNs2bKNLj/++ONzySWXZL/99ssHPvCBJMnNN9+cJUuWZI899sib3/zm\nB2aaN29efvCDH2TVqlXZa6+9cuyxx+YpT3lKnv/85+fuu+9Oklx++eV56lOfmv322++BP28mE3EA\nwGb1+c9/Po997GNz5ZVX5pprrsmSJUuSJLvvvnsmJiayaNGiHHXUUTnrrLNy2WWX5aSTTkqSnHPO\nOZmYmMiVV16ZCy64IEuXLs0tt9yyweXvfve7s2jRokxMTORNb3pTkmRiYiLLli3L1VdfnWXLluXG\nG2/8pfm+853v5A/+4A9y7bXXZpdddsnZZ5+dJDn66KPz93//95mYmMjs2bO30N/WgyfiAIDNat99\n980Xv/jFvOUtb8kll1ySnXfeOUly6KGHPvD6QQcdlB133DFz587NtttumzvuuCOXXnppjjzyyMye\nPTuPfvSj8+xnPzuXX375Bpevz+LFi7Pzzjtnu+22y957750bbvjlryGdP39+9ttvvyTJAQcckFWr\nVuWOO+7InXfemYMPPjhJ8qpXvWoUfzWblfvEAQCb1ZOf/OSsWLEi5513Xt761rdm8eLFSZJtt902\nSTJr1qwHHq99vmbNms3yZ0/e7uzZs9e73XXXWXs4tTf2xAEAm9XNN9+c7bffPq9+9auzdOnSrFix\nYkrvW7RoUZYtW5b7778/t912Wy6++OIceOCBG1y+44475s4779wsM++yyy7Zcccd87WvfS1JcuaZ\nZ26W7Y6SPXEAwGZ19dVXZ+nSpZk1a1bmzJmTD3/4wzniiCM2+b7DDz88X/3qV7NgwYJUVd773vfm\nMY95zAaX77rrrpk9e3YWLFiQo446Ko94xCN+pblPPfXUHHvssZk1a1ae/exnP3AYeKaq1tq4Zxi5\nhQsXtuXLl497DACYMa677rrstdde4x5jRrnrrruyww47JEne/e5355Zbbslf//Vfj/TPXN/nUFVX\ntNYWbuq99sQBACT53Oc+l3e9611Zs2ZNfuM3fiOnn376uEfaKBEHAJDkFa94RV7xileMe4wpc2ED\nAECHRBwAQIdEHABAh0QcAECHRBwAwDouuuiifOUrX3ng+drven2w3va2t+V973vf5hjtAa5OBQBy\nyBvetVm3d+GHTtis29vSLrroouywww75rd/6rXGPskEibh2b+3/EW0Lv/1AA2DqtWrUqL37xi3PN\nNdckSd73vvflrrvuykUXXZSDDjooX/rSl3LHHXfk1FNPzaJFi3Lttdfm6KOPzn333Zef//znOfvs\nszNnzpwsWbIkz3jGM/KVr3wlT3/603P00UfnpJNOyq233pqPf/zjOfDAA/PDH/4wxxxzTK6//vps\nv/32OeWUU/LUpz51vct32mmnnHzyyZk9e3Y+9rGP5YMf/GCS5OKLL8773//+fP/738973/veB76F\n4i/+4i/yyU9+Mvfee28OP/zw/Nmf/VmS5J3vfGfOOOOMPOpRj8rjH//4HHDAAZv178/hVABgxlmz\nZk2+/vWv56/+6q8eiKKTTz45f/RHf5SJiYksX748u+22W5Jk5cqV+e///b/nW9/6Vr71rW/lE5/4\nRC699NK8733vy5//+Z8nSU466aQ87WlPy1VXXZU///M/z2te85oNLp83b15+//d/P29605syMTGR\nRYsWJUluueWWXHrppfnsZz+b448/Pkly/vnn5zvf+U6+/vWvZ2JiIldccUUuvvjiXHHFFTnzzDMz\nMTGR8847L5dffvlm/zuyJw4AmHFe9rKXJUkOOOCArFq1Kkly8MEH553vfGdWr16dl73sZdljjz2S\nJPPnz8++++6bJHnKU56SxYsXp6qy7777PvDeSy+9NGeffXaS5JBDDsntt9+en/zkJxtcvj4vfelL\nM2vWrOy9997593//9ySDiDv//PPztKc9Lcngq7u+853v5M4778zhhx+e7bffPkly6KGHbua/IXvi\nAIAx2WabbfLzn//8gef33HPPA4+33XbbJMns2bOzZs2aJMmrXvWqnHvuuXn4wx+eF77whbnwwgt/\nYd0kmTVr1gPPZ82a9cB7N4fJf87a755vreWEE07IxMREJiYmsnLlyrzuda/bbH/mxog4AGAsHv3o\nR+fWW2/N7bffnnvvvTef/exnN7r+9ddfnyc84Qn5wz/8wxx22GG56qqrpvxnLVq0KB//+MeTDC5a\neOQjH5mddtppg8t33HHH3HnnnZvc7gte8IKcdtppueuuu5IkN910U2699db89m//dv75n/85d999\nd+6888585jOfmfKsU+VwKgAwFnPmzMmJJ56YAw88MI973OOy5557bnT9T37yk/noRz+aOXPm5DGP\neUz+x//4Hxs89Lmut73tbTnmmGPy1Kc+Ndtvv33OOOOMjS5/yUtekiOOOCKf/vSnH7iwYX2e//zn\n57rrrsvBBx+cJNlhhx3ysY99LPvvv39e8YpXZMGCBXnUox6Vpz/96VOaczpq7e7Ah7KFCxe25cuX\nT2ldV6cCsDW47rrrstdee417jK3e+j6HqrqitbZwU+91OBUAoEMiDgCgQyIOAKBDIg4AoEMiDgCg\nQyIOAKBDIg4AeMhatWpVPvGJTzzw/PTTT89xxx33oLd30UUX5cUvfvHmGO1X5ma/AEDeeNpFm3V7\nHzzmOZt1ew/W2oh71ateNe5RNjt74gCALe6nP/1pXvSiF2XBggXZZ599smzZssybNy8nnHBC9ttv\nvyxcuDArVqzIC17wgjzxiU/MySefnGTwXaVLly7NPvvsk3333TfLli3b6PLjjz8+l1xySfbbb798\n4AMfSJLcfPPNWbJkSfbYY4+8+c1vfmCm888/PwcffHD233//vPzlL3/gq7Q+//nPZ88998z++++f\nc845Z0v+NW2UiAMAtrjPf/7zeexjH5srr7wy11xzTZYsWZIk2X333TMxMZFFixblqKOOyllnnZXL\nLrssJ510UpLknHPOycTERK688spccMEFWbp0aW655ZYNLn/3u9+dRYsWZWJiIm9605uSJBMTE1m2\nbFmuvvrqLFu2LDfeeGN+8IMf5B3veEcuuOCCrFixIgsXLsz73//+3HPPPTn22GPzmc98JldccUW+\n//3vj+3vbF0iDgDY4vbdd9988YtfzFve8pZccskl2XnnnZMkhx566AOvH3TQQdlxxx0zd+7cbLvt\ntrnjjjty6aWX5sgjj8zs2bPz6Ec/Os9+9rNz+eWXb3D5+ixevDg777xztttuu+y999654YYbctll\nl+Wb3/xmnvnMZ2a//fbLGWeckRtuuCHf+ta3Mn/+/Oyxxx6pqrz61a/eYn9Hm+KcOABgi3vyk5+c\nFStW5Lzzzstb3/rWLF68OEmy7bbbJklmzZr1wOO1z9esWbNZ/uzJ2509e3bWrFmT1lqe97zn5Z/+\n6Z9+Yd2JiYnN8meOgj1xAMAWd/PNN2f77bfPq1/96ixdujQrVqyY0vsWLVqUZcuW5f77789tt92W\niy++OAceeOAGl++444658847N7ndZzzjGfnyl7+clStXJhmcs/dv//Zv2XPPPbNq1ap897vfTZJf\nirxxsicOANjirr766ixdujSzZs3KnDlz8uEPfzhHHHHEJt93+OGH56tf/WoWLFiQqsp73/vePOYx\nj9ng8l133TWzZ8/OggULctRRR+URj3jEerc7d+7cnH766TnyyCNz7733Jkne8Y535MlPfnJOOeWU\nvOhFL8r222+fRYsWTSkKt4RqrY17hpFbuHBhW758+ZTWPeQN7xrxNJvfhR86YdwjANCZ6667Lnvt\ntde4x9jqre9zqKorWmsLN/Veh1MBADok4gAAOiTiAAA6JOIAYCu1NZwXP5P9qn//Ig4AtkLbbbdd\nbr/9diE3Jq213H777dluu+0e9DbcYgQAtkK77bZbVq9endtuu23co2y1tttuu+y2224P+v0iDgC2\nQnPmzMn8+fPHPQa/AodTAQA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAA\nOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok\n4gAAOiTiAAA6JOIAADo00oirqiVV9e2qWllVx6/n9W2ratnw9a9V1bzh8udV1RVVdfXw90Mmveei\n4TYnhr8eNcqfAQBgJtpmVBuuqtlJ/i7J85KsTnJ5VZ3bWvvmpNVel+RHrbUnVdUrk7wnySuS/CDJ\nS1prN1fVPkm+kORxk973e6215aOaHQBgphvlnrgDk6xsrV3fWrsvyZlJDltnncOSnDF8fFaSxVVV\nrbVvtNZuHi6/NsnDq2rbEc4KANCVUUbc45LcOOn56vzi3rRfWKe1tibJj5Psus46v5tkRWvt3knL\nPjI8lPqnVVWbd2wAgJlvRl/YUFVPyeAQ6/87afHvtdb2TbJo+Ou/bOC9r6+q5VW1/Lbbbhv9sAAA\nW9AoI+6mJI+f9Hy34bL1rlNV2yTZOcntw+e7JflUkte01r679g2ttZuGv9+Z5BMZHLb9Ja21U1pr\nC1trC+fOnbtZfiAAgJlilBF3eZI9qmp+VT0sySuTnLvOOucmee3w8RFJLmyttaraJcnnkhzfWvvy\n2pWrapuqeuTw8ZwkL05yzQh/BgCAGWlkETc8x+24DK4svS7JJ1tr11bV26vq0OFqpybZtapWJvnj\nJGtvQ3JckiclOXGdW4lsm+QLVXVVkokM9uT9w6h+BgCAmWpktxhJktbaeUnOW2fZiZMe35Pk5et5\n3zuSvGMDmz1gc84IANCjGX1hAwAA6yfiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6\nJOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTi\nAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAA\nOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok\n4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIA\nADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6\nJOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTi\nAAA6JOIAADok4gAAOiTiAAA6JOIAADok4gAAOiTiAAA6NNKIq6olVfXtqlpZVcev5/Vtq2rZ8PWv\nVdW84fLnVdUVVXX18PdDJr3ngOHylVX1N1VVo/wZAABmopFFXFXNTvJ3SX4nyd5JjqyqvddZ7XVJ\nftRae1KSDyR5z3D5D5K8pLW2b5LXJvnopPd8OMmxSfYY/loyqp8BAGCmGuWeuAOTrGytXd9auy/J\nmUkOW2edw5KcMXx8VpLFVVWttW+01m4eLr82ycOHe+1+PclOrbXLWmstyT8meekIfwYAgBlpmxFu\n+3FJbpz0fHWSgza0TmttTVX9OMmuGeyJW+t3k6xord1bVY8bbmfyNh+3uQfvzRtPu2jcI0zbB495\nzrhHAICujTLifmVV9ZQMDrE+/0G89/VJXp8ku++++2aeDABgvEZ5OPWmJI+f9Hy34bL1rlNV2yTZ\nOcntw+e7JflUkte01r47af3dNrHNJElr7ZTW2sLW2sK5c+f+ij8KAMDMMsqIuzzJHlU1v6oeluSV\nSc5dZ51zM7hwIUmOSHJha61V1S5JPpfk+Nbal9eu3Fq7JclPquoZw6tSX5Pk0yP8GQAAZqSRRVxr\nbU2S45J8Icl1ST7ZWru2qt5eVYcOVzs1ya5VtTLJHydZexuS45I8KcmJVTUx/PWo4WtvSPK/kqxM\n8t0k/zKqnwEAYKYa6TlxrbXzkpy3zrITJz2+J8nL1/O+dyR5xwa2uTzJPpt3UgCAvvjGBgCADok4\nAIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACA\nDok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6J\nOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgA\ngA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADm0z7gFgczrkDe8a9wjTduGHThj3\nCAB0yJ44AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgA\ngA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAO\nbTPVFavqUUmemeSxSe5Ock2S5a21n49oNgAANmCTEVdVz01yfJL/lOQbSW5Nsl2SlyZ5YlWdleQv\nW2s/GeWgAAD8h6nsiXthkmNba//fui9U1TZJXpzkeUnO3syzAQCwAZuMuNba0o28tibJP2/WiQAA\n2KTpnBP33SSXJbkkySWttWtHNhUAABs1natT907y90l2TfIXVfXdqvrUaMYCAGBjphNx9yf52fD3\nn2dwgcOtoxgKAICNm/Lh1CQ/SXJ1kvcn+YfW2u2jGQkAgE2Zzp64I5NcnOQNSc6sqj+rqsWjGQsA\ngI2Z8p641tqnk3y6qvZM8jtJ/luSNyd5+IhmAwBgA6a8J66qzq6qlUn+Osn2SV6T5BGjGgwAgA2b\nzjlx70ryjdba/aMaBgCAqZnO4dTloxwEAICpm8p3p34vSZu8aAPP/6q19jebdzwAANZnKl+7NX9L\nDAIAwNRN5xYjSZKq+rWqmj2KYQAAmJpNRlxVzaqqV1XV56rq1iTfTvL9qvpmVf1FVT1p9GMCADDZ\nVPbEfSnJE5OckOQxrbXdWmtzkzwryWVJ3lNVrx7hjAAArGMqEfefW2v/s7V2VWvt52sXttZ+2Fo7\nu7X2u0mWre+NVbWkqr5dVSur6vj1vL5tVS0bvv61qpo3XL5rVX2pqu6qqr9d5z0XDbc5Mfz1qOn8\nwAAADwVTubDhZw9mneF5c3+X5HlJVie5vKrOba19c9Jqr0vyo9bak6rqlUnek+QVSe5J8qdJ9hn+\nWtfvueUJALA1m/aFDWtV1XXDX8dtYJUDk6xsrV3fWrsvyZlJDltnncOSnDF8fFaSxVVVrbWfttYu\nzSDmAABYx4OOuNbaXhmcF/e9DazyuCQ3Tnq+erhsveu01tYk+XGSXafwx39keCj1T6uqpjU4AMBD\nwIOOuCRprd3eWvvc5hpmin6vtbZvkkXDX/9lfStV1euranlVLb/tttu26IAAAKM2lVuMfK+qrq+q\nr01z2zclefyk57sNl613naraJsnOSW7f2EZbazcNf78zyScyOGy7vvVOaa0tbK0tnDt37jRHBwCY\n2TYZca21+a21J7TWDprmti9PskdVza+qhyV5ZZJz11nn3CSvHT4+IsmFrbWWDaiqbarqkcPHc5K8\nOMk105wLAKB7m7w6dbKqelaSPVprH6mquUl2aK2t95y41tqa4UUPX0gyO8lprbVrq+rtSZa31s5N\ncmqSj1bVyiQ/zCD01v5Zq5LslORhVfXSJM9PckOSLwwDbnaSC5L8w7R+YgCAh4ApR1xVnZRkYZLf\nTPKRJHOSfCzJMzf0ntbaeUnOW2fZiZMe35Pk5Rt477wNbPaAqc4MAPBQNZ0LGw5PcmiSnyZJa+3m\nJDuOYigAADZuOhF33/B8tZYkVfVroxkJAIBNmU7EfbKq/j7JLlV1bJyPBgAwNlM+J6619r6qel6S\nn2RwXtyJrbUvjmwyAAA2aFpXpw6jTbgBAIzZlA+nVtXLquo7VfXjqvpJVd1ZVT8Z5XAAAKzfdPbE\nvTfJS1pr141qGAAApmY6Fzb8u4ADAJgZNrknrqpeNny4vKqWJfnnJPeufb21ds6IZgMAYAOmcjj1\nJZMe/98Mvv5qrZZExAEAbGGbjLjW2tFJUlXPbK19efJrVbXBr9wCAGB0pnNO3AenuAwAgBGbyjlx\nByf5rSRzq+qPJ720U5LZoxoMAIANm8o5cQ9LssNw3clfeP+TJEeMYigAADZuKufE/Z8k/6eqTm+t\n3bAFZgIAYBOmcjj1IxlchfrjJG8a+UQAAGzSVA6nnj78/b4RzgEAwDRM9XAqAAAzyFQOp34mySlJ\nPt9a+9k6rz0hyVFJVrXWThvJhMBW45A3vGvcI0zLhR86YdwjAFuxqRxOPTbJHyf5q6r6YZLbkmyX\nZF6S7yb529bap0c2IQAAv2Qqh1O/n+TNSd5cVfOS/HqSu5P8W2vt/450OgAA1msqh1OrtdaSpLW2\nKsmqja0DAMDoTeVrt75UVW+sqt0nL6yqh1XVIVV1RpLXjmY8AADWZyrnxC1JckySfxpeyPCjDM6J\nm53k/CR/1Vr7xuhGBABgXVM5J+6eJB9K8qGqmpPkkUnubq3dMerhAABYv6mcE7ddkt9P8qQkVyU5\nrbW2ZtSDAQCwYVM5J+6MJAuTXJ3khUn+cqQTAQCwSVM5J27v1tq+SVJVpyb5+mhHAgBgU6YScQ98\nS0NrbU1VjXAcAHrU27dtJL5xg/5NJeIWVNVPho8rycOHzytJa63tNLLpAABYr6lcnTp7SwwCAMDU\nTeXCBgAAZhgRBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA0CERBwDQ\noU1+dyowWm887aJxjzAtHzzmOeMeAYDYEwcA0CURBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA\n0CERBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA0CERBwDQIREHANAh\nEQcA0CERBwDQIREHANAhEQcA0CERBwDQIREHANAhEQcA0CERBwDQoW3GPQAAMF6HvOFd4x5hWi78\n0AnjHmFGsCcOAKBDIg4AoEMiDgCgQyIOAKBDIg4AoEMiDgCgQyIOAKBDIg4AoEMiDgCgQyIOAKBD\nIg4AoEMiDgCgQyIOAKBDI424qlpSVd+uqpVVdfx6Xt+2qpYNX/9aVc0bLt+1qr5UVXdV1d+u854D\nqurq4Xv+pqpqlD8DAMBMNLKIq6rZSf4uye8k2TvJkVW19zqrvS7Jj1prT0rygSTvGS6/J8mfJvmT\n9Wz6w0mOTbLH8NeSzT89AMDMNso9cQcmWdlau761dl+SM5Mcts46hyU5Y/j4rCSLq6paaz9trV2a\nQcw9oKp+PclOrbXLWmstyT8meekIfwYAgBlplBH3uCQ3Tnq+erhsveu01tYk+XGSXTexzdWb2CYA\nwEPeQ/bChqp6fVUtr6rlt91227jHAQDYrEYZcTclefyk57sNl613naraJsnOSW7fxDZ328Q2kySt\ntVNaawtbawvnzp07zdEBAGa2UUbc5Un2qKr5VfWwJK9Mcu4665yb5LXDx0ckuXB4rtt6tdZuSfKT\nqnrG8KrU1yT59OYfHQBgZttmVBtura2pquOSfCHJ7CSntdauraq3J1neWjs3yalJPlpVK5P8MIPQ\nS5JU1aokOyV5WFW9NMnzW2vfTPKGJKcneXiSfxn+Atji3njaReMeYdo+eMxzxj0CsJmMLOKSpLV2\nXpLz1ll24qTH9yR5+QbeO28Dy5cn2WfzTQkA0J+H7IUNAAAPZSIOAKBDIg4AoEMiDgCgQyIOAKBD\nI706FQBmqt5uEeP2MKzLnjgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4\nAIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACA\nDok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA5tM+4BAACm442nXTTuEabt\ng8c8Z7Nv0544AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAO\niTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4\nAIAOiTgAgA6JOACADok4AIAHYqZ3AAAN6ElEQVQOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4\nAIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACA\nDok4AIAOiTgAgA6JOACADok4AIAOiTgAgA6JOACADo004qpqSVV9u6pWVtXx63l926paNnz9a1U1\nb9JrJwyXf7uqXjBp+aqqurqqJqpq+SjnBwCYqbYZ1YaranaSv0vyvCSrk1xeVee21r45abXXJflR\na+1JVfXKJO9J8oqq2jvJK5M8Jcljk1xQVU9urd0/fN9zW2s/GNXsAAAz3Sj3xB2YZGVr7frW2n1J\nzkxy2DrrHJbkjOHjs5IsrqoaLj+ztXZva+17SVYOtwcAQEYbcY9LcuOk56uHy9a7TmttTZIfJ9l1\nE+9tSc6vqiuq6vUjmBsAYMYb2eHUEXpWa+2mqnpUki9W1bdaaxevu9Iw8F6fJLvvvvuWnhEAYKRG\nuSfupiSPn/R8t+Gy9a5TVdsk2TnJ7Rt7b2tt7e+3JvlUNnCYtbV2SmttYWtt4dy5c3/lHwYAYCYZ\nZcRdnmSPqppfVQ/L4EKFc9dZ59wkrx0+PiLJha21Nlz+yuHVq/OT7JHk61X1a1W1Y5JU1a8leX6S\na0b4MwAAzEgjO5zaWltTVccl+UKS2UlOa61dW1VvT7K8tXZuklOTfLSqVib5YQahl+F6n0zyzSRr\nkvxBa+3+qnp0kk8Nrn3INkk+0Vr7/Kh+BgCAmWqk58S11s5Lct46y06c9PieJC/fwHvfmeSd6yy7\nPsmCzT8pAEBffGMDAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0Qc\nAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABA\nh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdE\nHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwA\nQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECH\nRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0Qc\nAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdEHABA\nh0QcAECHRBwAQIdEHABAh0QcAECHRBwAQIdGHnFVtaSqvl1VK6vq+PW8vm1VLRu+/rWqmjfptROG\ny79dVS+Y6jYBAB7qRhpxVTU7yd8l+Z0keyc5sqr2Xme11yX5UWvtSUk+kOQ9w/funeSVSZ6SZEmS\nD1XV7CluEwDgIW3Ue+IOTLKytXZ9a+2+JGcmOWyddQ5Lcsbw8VlJFldVDZef2Vq7t7X2vSQrh9ub\nyjYBAB7SRh1xj0ty46Tnq4fL1rtOa21Nkh8n2XUj753KNgEAHtK2GfcAo1JVr0/y+uHTu6rq2+Oc\nZ5S+lDwyyQ/GPcd0/O3rxj3BzNHb5+ez+w+9fXaJz2+y3j4/n91/6O2zS6b9+f3GVFYadcTdlOTx\nk57vNly2vnVWV9U2SXZOcvsm3rupbaa1dkqSU36V4XtRVctbawvHPQcPjs+vXz67vvn8+uWzGxj1\n4dTLk+xRVfOr6mEZXKhw7jrrnJvktcPHRyS5sLXWhstfObx6dX6SPZJ8fYrbBAB4SBvpnrjW2pqq\nOi7JF5LMTnJaa+3aqnp7kuWttXOTnJrko1W1MskPM4iyDNf7ZJJvJlmT5A9aa/cnyfq2OcqfAwBg\npqnBTi96VlWvHx4+pkM+v3757Prm8+uXz25AxAEAdMjXbgEAdEjEAQB0SMQBAHRIxMEWVgOvrqoT\nh893r6oDxz0Xm1ZVfzSVZcxMVXVOVb2oqvx/X4f8+/tlLmzoSFV9MMkGP7DW2h9uwXF4kKrqw0l+\nnuSQ1tpeVfWIJOe31p4+5tHYhKpa0Vrbf51l32itPW1cMzF1VfWfkxyd5BlJ/neSj7TWHrLf5vNQ\n49/fL3vIfu3WQ9Ty4e/PTLJ3kmXD5y/P4H569OGg1tr+VfWNJGmt/Wh442pmqKo6Msmrksyvqsk3\nF98pg/tb0oHW2gVJLqiqnZMcOXx8Y5J/SPKx1trPxjog6+Xf34aJuI601s5Ikqr6r0me1VpbM3x+\ncpJLxjkb0/Kzqpqd4V7VqpqbwZ45Zq6vJLklg+9r/MtJy+9MctVYJuJBqapdk7w6yX9J8o0kH0/y\nrAy+Oeg545uMjfDvbwNEXJ8ekV/8L5Adhsvow98k+VSSR1XVOzP4urm3jnckNqa1dkOSG4aH4+5u\nrf28qp6cZM8kV493Oqaqqj6V5DeTfDTJS1prtwxfWlZVyzf8TsbJv78Nc05ch6rq6CRvS/KlJJXk\nt5O8be2eOma+qtozyeIMPr9/ba1dN+aRmIKquiLJogz+o+nLGXyX832ttd8b62BMSVU9t7X2pXHP\nwYPj398vE3GdqqrHJDlo+PRrrbXvj3Mepmd4McPjM2lveGttxfgmYirWnlhdVW9M8vDW2nuraqK1\ntt+4Z2NqqmqfDM4p3m7tstbaP45vIqbKv79f5nBqR6pq/3UW3Tj8/bFV9VgR0Ieq+p9Jjkry3fzH\n1cYtySHjmokpq6o6OMnvJXndcNnsMc7DNFTVSRmc97Z3kvOS/E6SS5OIuD7497cOEdeXv9zIayKg\nH/9Pkie21u4b9yBM2x8lOSHJp1pr11bVEzI4rYE+HJFkQZJvtNaOrqpHJ/nYmGdi6v5b/Pv7BQ6n\ndmZ4k8qDW2tfHvcsPDhVdXaS/9pau3XcszB1wyuK39Na+5Nxz8KDU1WXt9aePjy36rkZXN14XWtt\nzzGPxjRU1Q5J0lq7a9yzjJs9cZ0ZXpXzt0m22psbPgS8K8k3quqaJPeuXdhaO3R8I7EprbX7q+pZ\n456DX8nlVbVLBveFuyLJXUm+Ot6RmKqq2jeDQ9//afC0bkvymtbateOdbHxEXJ/+tap+N8k5za7U\nHp2R5D0ZXBrv/nB9+cbwZqP/O8lP1y5srZ0zvpGYhp0yuDn6RUk+n2Sn1tpWfZ+xzvx9kj9ee4Vx\nVT0ngyD/rXEONU4Op3aoqu5M8mtJ7k9ydwa3qWittZ3GOhhTsvaQzrjnYPqq6iPrWdxaa8ds8WGY\ntqp6bga3qFiU5IkZ3Oz34tbaX491MKakqq5srS3Y1LKtiYiDLayq3p/BYdRz84uHU11dDCM2PLfx\n6RmcE/f7Gdw81jlxHRjerHlFBjdrTgbfvHFAa+3w8U01XiKuU1V1aAY3+U2Si1prnx3nPExdVa3v\naqrWWnN18QxVVW8e3pPqg/mP28I8oLX2h2MYi2mqqn/N4CjGVzP4qsJLXWDUj+H9Nf8sg+8PTwaf\n4dtaa3eMb6rxck5ch6rq3Rn8l+THh4v+qKqe2Vo7YYxjMUWtteeOewam7S1J3pvBvf1+NOZZePCu\nSnJAkn2S/DjJHVX11dba3eMdiyl6YgY3SZ+VQb8szuDWWk8d51DjZE9ch6rqqiT7tdZ+Pnw+O4P7\nHm21/0PuTVW9KMlT8ot3jX/7+CZiY6rqm0n+c5J/yeBmsTX59dbaD9fzNmaoqtoxgxtu/0mSx7TW\nth3vRExFVX07g8/smky6KGz43apbJXvi+rVLkrX/x7HzOAdheqrq5CTbZ3BOzv/K4AakXx/rUGzK\nh5P8a5InZHBrirUqg8OrTxjHUExPVR2XwUUNByRZleS0DA7J0YfbWmufGfcQM4k9cR2qqlcmeXcG\nl8lXBufGHd9aWzbOuZiaqrqqtfbUSb/vkORfWmuLxj0bG1dVH26t/ddxz8GDU1V/kkG0XdFaWzPu\neZieqlqc5MgM/oNq8kVhW+0tfkRch6rqY0n+LYNzc1Yluby19v2xDsWUVdXXW2sHVtVlSV6WwR7V\na1prTxrzaAAz1vD/+/ZMcm3+43DqVn2LH4dT+3RqBocEDs3wXkdV5V5H/fjM8K7xf5HB5fItgxtW\nArBhT2+t/ea4h5hJRFyHWmtfqqqL84v3OnpKEhHXh28lub+1dnZV7Z1k/yT/POaZAGa6r1TV3q21\nb457kJnC4dQOuddR3yadC/esJP8zyfuSnNhaO2jMowHMWFV1XQZHn76XwTlxa7+taKu9M4M9cX1y\nr6O+3T/8/UVJ/qG19rmqesc4BwLowJJxDzDT2BPXMfc66lNVfTbJTUmel8Gh1LuTfH1r/v4/AKZP\nxHVoPfc6uiTJJa21C8c5F1NTVdtn8F+UV7fWvlNVv55k39ba+WMeDYCOiLgOudcRACDiAAA6NGvc\nAwAAMH0iDgCgQyIOAKBDIg4AoEMiDmAaqmppVf3h8PEHqurC4eNDqurj450O2JqIOIDpuSSD+zQm\nycIkO1TVnOGyi8c2FbDVEXEA03NFkgOqaqcMvr/xqxnE3KIMAg9gi/DdqQDT0Fr7WVV9L4OvvPtK\nBt9l/NwkT0py3RhHA7Yy9sQBTN8lGXxn8cXDx7+f5BvN3dOBLUjEAUzfJUl+PclXW2v/nuSeOJQK\nbGG+dgsAoEP2xAEAdEjEAQB0SMQBAHRIxAEAdEjEAQB0SMQBAHRIxAEAdEjEAQB06P8H9NMw5271\nWCoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6826b2f898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJlCAYAAACFRrWUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQCgY/8/6wxqZ5TAzo4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67fe85dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "word = ('the', )\n",
    "plot_bigram_dist(word, bigram, bigram_smoothed, k=5)\n",
    "\n",
    "word = ('environments', )\n",
    "plot_bigram_dist(word, bigram, bigram_smoothed, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Recall** that if we have a sentence $w_1,\\dots,w_n$ we can write\n",
    "\n",
    "$$P(w_1\\dots w_n) = P(w_1)P(w_2|w_1) \\cdots P(w_i|w_1 \\dots w_{n-1}) \\approx P(w_1)P(w_2|w_1)\\cdots P(w_{N-1}|w_1\\dots w_{N-2})\\prod_{i=N}^{n} P(w_i|w_{i-(N-1)}\\dots w_{i-1})$$\n",
    "\n",
    "where in the last step we make an $N$-gram approximation of the full conditionals.\n",
    "\n",
    "For example, in the case of a bigram (N=2), the above expression reduces to\n",
    "\n",
    "$$P(w_1 \\dots w_n)\\approx P(w_1)\\prod_{i=2}^{n} P(w_i| w_{i-1}).$$\n",
    "\n",
    "## Exercise 2.4 (5 points)\n",
    "\n",
    "The following sentences are taken from the **training data**. Use your **unsmoothed unigram**, **bigram**, and **trigram** language model to estimate their **probabilities**:\n",
    "\n",
    "    1. Every day was about creating something new .\n",
    "    2. In this machine , a beam of protons and anti-protons are accelerated to near the speed of light and brought \n",
    "       together in a collision , producing a burst of pure energy .\n",
    "\n",
    "**Repeat** this with the **smoothed (add-1)** versions of the N-grams. What is the effect of smoothing on the probabilities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "trigram, unigram = train_ngram(data, N=3, k=0)\n",
    "trigram_smoothed, unigram_smoothed = train_ngram(data, N=3, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1 without smoothing: \n",
      "1-gram probability: 2.6727914595401075e-24\n",
      "2-gram probability: 4.667998867880972e-17\n",
      "3-gram probability: 1.241092166741438e-11\n",
      "\n",
      "Sentence1 with smoothing: \n",
      "1-gram probability: 2.4178740677475678e-24\n",
      "2-gram probability: 1.7626843091651718e-26\n",
      "3-gram probability: 1.3301406368835432e-32\n",
      "\n",
      "Sentence2 without smoothing: \n",
      "1-gram probability: 2.779108517050293e-100\n",
      "2-gram probability: 2.0159953795734388e-72\n",
      "3-gram probability: 3.570432747502253e-31\n",
      "\n",
      "Sentence2 with smoothing: \n",
      "1-gram probability: 4.3471454835405504e-100\n",
      "2-gram probability: 2.0095422945886214e-116\n",
      "3-gram probability: 5.334908390126462e-134\n"
     ]
    }
   ],
   "source": [
    "def compute_sentence_probability(sentence, gram, N):\n",
    "    sentence = sentence.split(' ')\n",
    "    original_sentence = sentence\n",
    "    #for N, ngram in grams.items():\n",
    "    sentence = original_sentence\n",
    "    if N >= 2:\n",
    "        for n in range(N-1):\n",
    "            sentence = ['<s>'] + sentence\n",
    "    #print(sentence)\n",
    "    probability = 1        \n",
    "    for idx_word, word in enumerate(sentence):\n",
    "        #print(word)            \n",
    "        tuple_list = []\n",
    "        window_size = N\n",
    "        if idx_word < window_size - 1:\n",
    "            continue\n",
    "        window_start = idx_word - window_size + 1\n",
    "        window_end = idx_word\n",
    "\n",
    "        for j in range(window_start, window_end, 1):               \n",
    "            tuple_list.append(sentence[j])          \n",
    "\n",
    "        tuple_words = tuple(tuple_list)        \n",
    "        \n",
    "        if N == 1:\n",
    "            probability *= gram[word]\n",
    "        else:                \n",
    "            #print(tuple_words)\n",
    "            #print(gram[tuple_words][word])\n",
    "            probability *= gram[tuple_words][word]\n",
    "    return probability\n",
    "    \n",
    "    #print()\n",
    "    \n",
    "sentence1 = 'Every day was about creating something new .'\n",
    "grams = {1: unigram, 2: bigram, 3: trigram}\n",
    "grams_smoothed = {1: unigram_smoothed, 2: bigram_smoothed, 3: trigram_smoothed}                \n",
    "\n",
    "print(\"Sentence1 without smoothing: \")\n",
    "for N, gram in grams.items():\n",
    "    probability = compute_sentence_probability(sentence1, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "\n",
    "print()\n",
    "    \n",
    "print(\"Sentence1 with smoothing: \")\n",
    "for N, gram in grams_smoothed.items():\n",
    "    probability = compute_sentence_probability(sentence1, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "\n",
    "print()\n",
    "    \n",
    "sentence2 = 'In this machine , a beam of protons and anti-protons are accelerated to near the \\\n",
    "speed of light and brought together in a collision , producing a burst of pure energy .'\n",
    "\n",
    "print(\"Sentence2 without smoothing: \")\n",
    "for N, gram in grams.items():\n",
    "    probability = compute_sentence_probability(sentence2, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "    \n",
    "print()\n",
    "    \n",
    "print(\"Sentence2 with smoothing: \")\n",
    "for N, gram in grams_smoothed.items():\n",
    "    probability = compute_sentence_probability(sentence2, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.5 (5 points)\n",
    "\n",
    "The above sentences were taken from the training set, hence they will all have probability greater than 0. The big challenge for our language model are of course with sentence that contain unseen N-grams: if such an N-gram occurs our model immediately assigns the sentence probability zero.\n",
    "\n",
    "The following three senteces are taken from the **test set** availlable in the file **ted-test.txt**. What probabilities do your smoothed and unsmoothed language models asign in this case?\n",
    "\n",
    "    1. Because these robots are really safe .\n",
    "    2. We have sheer nothingness on one side , and we have this vision of a reality that encompasses every \n",
    "       conceivable world at the other extreme : the fullest possible reality , nothingness , the simplest possible \n",
    "       reality ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = \"ted-test.txt\"\n",
    "test_data, w2i, i2w = read(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram probability: 1.1482902575109363e-21\n",
      "2-gram probability: 2.6913665324688018e-15\n",
      "3-gram probability: 0.0\n",
      "1-gram probability: 1.0534356800803786e-21\n",
      "2-gram probability: 9.379248193377376e-24\n",
      "3-gram probability: 2.433779391360207e-30\n",
      "1-gram probability: 2.8266525866764872e-118\n",
      "2-gram probability: 0.0\n",
      "3-gram probability: 0.0\n",
      "1-gram probability: 2.9022194676384047e-118\n",
      "2-gram probability: 1.356862482398554e-126\n",
      "3-gram probability: 0.0\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "sentence1 = 'Because these robots are really safe .'\n",
    "for N, gram in grams.items():\n",
    "    probability = compute_sentence_probability(sentence1, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "\n",
    "for N, gram in grams_smoothed.items():\n",
    "    probability = compute_sentence_probability(sentence1, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))    \n",
    "    \n",
    "#compute_sentence_probability(sentence1, grams_smoothed)\n",
    "\n",
    "sentence2 = 'We have sheer nothingness on one side , and we have this vision of a reality that encompasses every \\\n",
    "conceivable world at the other extreme : the fullest possible reality , nothingness , the simplest possible \\\n",
    "reality .'\n",
    "\n",
    "for N, gram in grams.items():\n",
    "    probability = compute_sentence_probability(sentence2, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "\n",
    "for N, gram in grams_smoothed.items():\n",
    "    probability = compute_sentence_probability(sentence2, gram, N)\n",
    "    print(str(N) + '-gram probability: ' + str(probability))\n",
    "    \n",
    "#compute_sentence_probability(sentence2, grams)\n",
    "#compute_sentence_probability(sentence2, grams_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional]\n",
    "\n",
    "**Optional** What percentage of the sentences in the test set get assigned probability 0 under your smoothed and unsmoothed language models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.6 (5 points)\n",
    "\n",
    "**Perplexity** is very frequently used **metric** for evaluating probabilistic models such as language models. The perplexity (sometimes called **PP** for short) of a language model on a sentence is the **inverse probability** of the sentence, **normalized** by the number of words:\n",
    "\n",
    "$$PP(w_1 \\dots w_n) = P(w_1\\dots w_n)^{-\\frac{1}{n}}.$$\n",
    "\n",
    "Here we can again approximate $P(w_1 \\dots w_n)$ with N-gram probabilities, as above.\n",
    "Note: $(x_1\\cdots x_n)^{-\\frac{1}{n}}$ is the **geometric mean** of the numbers $x_1,\\dots,x_n$. It is like the (regular) artithmetic mean, but with **products** instead of **sums**. The geometric mean is a more natural choice in the case of *PP* because behind $P(w_1\\dots w_n)$ is a series of $n$ products ([more here](https://en.wikipedia.org/wiki/Geometric_mean)).\n",
    "\n",
    "\n",
    "\n",
    "Compute the perplexity of the training sentences from excercise 2.1. What big difference between the **probabilities** of the sentences and the **perplexities** of the sentences do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "'''\n",
    "def train_ngram_perplexity(data, N, k=0):\n",
    "    \n",
    "    ngram = defaultdict(Counter) # ngram[history][word] = #(history,word)\n",
    "    if N - 2 > 0:\n",
    "        for sent_idx, sent in enumerate(data):\n",
    "            for i in range(0, N-2):\n",
    "                data[sent_idx].append(\"<s>\")\n",
    "    unpacked_data = [word for sent in data for word in sent]\n",
    "    unigram = defaultdict(float, Counter(unpacked_data)) # default prob is 0.0    \n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    V = len(unigram)\n",
    "    num_of_word_tokens = len(unpacked_data)\n",
    "    for i in range(N-1, len(unpacked_data)):   \n",
    "        #TODO remove this\n",
    "        #if i > 1000:\n",
    "        #    break\n",
    "        context = tuple(unpacked_data[i - N + 1:i])\n",
    "        ngram[context][unpacked_data[i]] += 1\n",
    "        \n",
    "    for context, count in ngram.items():\n",
    "        total = sum(count.values())\n",
    "        for word in count:\n",
    "            count[word] += k\n",
    "            count[word] /= float(total+k*V)\n",
    "            np.power(count[word], -1/total)\n",
    "        #count_sum = sum(count.values())\n",
    "        #for word in count:\n",
    "        #    count[word] /= count_sum +\n",
    "        ngram[context] = defaultdict(lambda total=total, k=k, V=V: np.power(k/float(total+k*V), -1/total), ngram[context])\n",
    "                    \n",
    "    for context, count in unigram.items():\n",
    "        unigram[context] += k\n",
    "        unigram[context] /= float(num_of_word_tokens + k*V)\n",
    "        np.power(count[word], -1/total)\n",
    "        \n",
    "    unigram = defaultdict(lambda num_of_word_tokens=num_of_word_tokens, k=k, V=V: np.power(k/float(num_of_word_tokens+k*V)), unigram)\n",
    "    return ngram, unigram\n",
    "\n",
    "#data, w2i, i2w = read(train_file)\n",
    "bigram_perplexity, unigram_perplexity = train_ngram(data, N=2, k=0)\n",
    "bigram_smoothed_perplexity, unigram_smoothed_perplexity = train_ngram(data, N=2, k=1)'''\n",
    "#grams = {2: bigram}\n",
    "#grams_smoothed = {2: bigram_smoothed} \n",
    "perplexity = []\n",
    "normal_probability = []\n",
    "count = 0\n",
    "for sentence in data:        \n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "    sentence_str = ''\n",
    "    #print(sentence)\n",
    "    for word in sentence:\n",
    "        if word == '<s>' or word == '</s>':\n",
    "            continue\n",
    "        sentence_str += word + ' '\n",
    "    sentence_str = sentence_str[:-1]\n",
    "    probability = compute_sentence_probability(sentence_str, bigram_smoothed, 2)    \n",
    "    normal_probability += [probability]    \n",
    "    perplexity += [probability ** (1 / len(sentence))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: \n",
      "[0.0017834827952319637, 0.0004117950929036414, 0.004734512063930554, 0.002370264464460582, 0.005629758668100014]\n",
      "\n",
      "Probability: \n",
      "[1.9130358308954124e-69, 2.328247673425317e-85, 3.0177194890110257e-40, 7.481419850163536e-14, 1.8004769297873189e-25]\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: ')\n",
    "print(perplexity[0:5])\n",
    "print()\n",
    "print('Probability: ')\n",
    "print(normal_probability[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "Congratulations, you have made it to the end of the tutorial. Here we will recap the gist of this notebook. \n",
    "\n",
    "**Make sure all your cells can be executed and all your answers are there. Then, read on if you're interested!**\n",
    "\n",
    "-----\n",
    "\n",
    "By now you should have a solid feeling for the problem of **sparsity in language data**; there's just never enough data. For the task of language modelling, we saw that sparsity is a serious challenge. \n",
    "\n",
    "It would be great to be able to model $p(w_n|w_1 \\dots w_{n-1})$ for unlimited $n$: the larger $n$ the better our language model should become at capturing the long-range dependencies between words that characterize actual human sentences, and the more probability our model will asign to such sentences as opposed to sentences that are word-soup. But in the N-gram approach, increasing $n$ will quickly kill all generalizing abilities of the model: the model will start to asign probabilities only to sentences it has seen in the training data.\n",
    "\n",
    "So, where to go from here? Here are three directions that we could head in.\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "We have seen one example of smoothing in this lab: add-k smoothing. This is an easy method, both conceptually and implementation-wise. But the results are not great, and the effects it has on the distributions can be extreme.\n",
    "\n",
    "A much more sophisticated method of smoothing is so-called **Kneser-Ney smoothing**. The method is described in detail in section 4.5 of J&M (3rd edition). This is one of the best performing N-gram smoothing methods, and up to a few years ago a popular implementation of it called [KenLM](https://kheafield.com/code/kenlm/) gave state of the art results.\n",
    "\n",
    "### From words to characters\n",
    "\n",
    "In this lab we have considered language modeling as the task of predicting a **word** $w_n$ based on a history of **words** $w_1\\cdots w_n$. What if instead we let our basic units of modelling be **characters**? The task then becomes to model $p(c_k\\mid c_{k-N-1}\\dots c_{k-1})$ where each $c_i$ is now an ASCII character instead of an entire word.\n",
    "\n",
    "Suddenly sparsity of data is no longer a problem! The set of characters to use is tiny (< 100) compared to even a small-sized vocabulary as today. Have a look at this very illustrative notebook written by Yoav Golberg to see such a method in action: [The unreasonable effectiveness of Character-level Language Models](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139).\n",
    "\n",
    "(So what is the downside?)\n",
    "\n",
    "\n",
    "### Neural language models\n",
    "\n",
    "The above notebook was actually written as a response to this blog post by Andrej Karpathy: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Go ahead and read it if you haven't already: it is a superb introduction to the topic of Recurrent Neural Networks.\n",
    "\n",
    "Neural language models solve the problem of data sparsity in a different manner. Instead of estimating the probabilities $p(w_k\\mid w_{k-N-1}\\dots w_{k-1})$ by counting occurences in the data, they use a neural network $f_{\\theta}$ parametrized by parameters $\\theta$ to predict this probability. The parameters $\\theta$ are learned through optimization. \n",
    "\n",
    "The simplest approach goes like this: each word in the history $w_{k-N-1}\\dots w_{k-1}$ is embedded separately giving  vectors $e_{k-N-1}\\dots e_{k-1}$ and then concatenated into one long vectors $[e_{k-N-1};\\dots ;e_{k-1}]$. The network then uses this history vector to predict a probability distribution over words $w$ in the vocabulary $V$:\n",
    "\n",
    "$$p(w \\mid w_{k-N-1}\\dots w_{k-1}) = f_{\\theta}([e_{k-N-1};\\dots;e_{k-1}]).$$\n",
    "\n",
    "(In order to produce legitimate probabilities the final layer of such a network will be for example a $softmax$.)\n",
    "\n",
    "This provides a solution to the sparsity problem by having the network let the individual embeddings of the words in the history interact through its non-linear transforamtion. We are letting the network figure out the smoothing itself!\n",
    "\n",
    "RNNs are a clever extension of this idea, where a hidden state vector $h$ is re-used and updated at each step $k$ in order to store the information of the entire history up to step $k-1$. That is, an RNN actually does away with the N-order approximation; it tries to model the full conditional directly! That means that\n",
    "\n",
    "$$p(w \\mid w_1\\dots w_{k-1}) \\approx RNN_{\\theta}([e_{k-1};h_{k-1}])$$\n",
    "\n",
    "where the hidden state $h_{k-1}$ is a compression of the *entire history* $w_1\\dots w_{k-1}$.\n",
    "\n",
    "Another great place to learn about RNNs, their problems, and solutions to those, is on the blog of [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). The project on language modelling will involve learning more about these methods. \n",
    "\n",
    "-----------\n",
    "(And now, it's time to read the classic essay by Eugene Wigner that gave both of the posts their title: [The Unreasonable Effectiveness of Mathematics in the Natural Sciences](http://www.dartmouth.edu/~matc/MathDrama/reading/Wigner.html))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
