{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Pointwise Mutual Information\n",
    "\n",
    "This lab is about applications of _Pointwise Mutual Information_ (PMI) in natural language processing.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Find **collocations** with PMI.\n",
    "2. Create **dense vector representations** for words (also known as **word embeddings**) by taking the singular value decomposition of a PMI co-occurence matrix.\n",
    "\n",
    "### Rules\n",
    "* The lab exercises should be made in **groups of two people**.\n",
    "\n",
    "* The deadline is **Sunday 10 Dec 23:59**.\n",
    "\n",
    "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**.\n",
    "\n",
    "* The **filename** should be `lab4_lastname1_lastname2.ipynb`, so for example `lab4_Levy_Goldberg.ipynb`.\n",
    "\n",
    "* The notebook is graded on a scale of **0-100**. The number of points for each question is indicated in parantheses. \n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should **write your code and answers in this iPython Notebook** (see http://ipython.org/notebook.html for reference material). If you have problems, please contact your teaching assistant.\n",
    "\n",
    "* Use only **one cell for code** and **one cell for markdown** answers!    \n",
    "\n",
    "    * Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
    "    \n",
    "    * For theoretical question, put your solution in the YOUR ANSWER HERE cell.\n",
    "    \n",
    "* Test your code and **make sure we can run your notebook**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI\n",
    "\n",
    "**Point-wise mutual information** is a measure of association used in information theory and statistics. In the words of [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/15.pdf):\n",
    "\n",
    "> The point-wise mutual information is a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent.\n",
    "\n",
    "It is formally defined as follows. Let $x$ and $y$ be outcomes of discrete random variables $X$ and $Y$. Then the point-wise mutual information between $x$ and $y$ is \n",
    "\n",
    "$$PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}.$$\n",
    "\n",
    "The values for $PMI$ range between minus and plus infinity. Large positive values indicate that $x$ and $y$ are strongly associated. A $PMI$ of zero means $x$ and $y$ are completely independent. And because of the logarithm, the values can also be negative, which indicates that $x$ and $y$ are co-occurring _less often_ than we would expect by chance. Negative values tend to be unreliable though, often resulting from the lack of coocurence statistics for $x$ and $y$. A more robust measure therefore is a variant of $PMI$ called the $Positive$ $PMI$:\n",
    "\n",
    "$$PPMI(x,y) = \\max(0, PMI(x, y)).$$\n",
    "\n",
    "In NLP applications, the $x$ and $y$ in the definition above are generally words, and the distributions based on their occurence in a text-corpus. Therefore from now on, let's refer to $x$ and $y$ as $w_i$ and $w_j$.\n",
    "\n",
    "----- \n",
    "\n",
    "You can read more about $PMI$ and $PPMI$ in the following sources:\n",
    "\n",
    "* [Jurafsky and Martin 15.2](https://web.stanford.edu/~jurafsky/slp3/15.pdf) is dedicated to $PMI$\n",
    "* The [Wikipedia entry on PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (it even has a language-related application of $PMI$)\n",
    "\n",
    "----- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence statistics\n",
    "\n",
    "To compute the $PMI$ of two words $w_i$ and $w_j$ we need the probabilities $p(w_i)$, $p(w_j)$ and $p(w_i,w_j)$. What are they?\n",
    "\n",
    "### Unigram\n",
    "\n",
    "The probabilites $p(w_i)$ and $p(w_j)$ are the probabilities of the words occuring by themselves in the corpus: they are the unigram probabilities of $w_i$ and $w_j$. We have seen before how to get these.\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "The probability $p(w_i, w_j)$ is the probability of $w_i$ and $w_j$ co-occuring together in the corpus. Co-occurrence is often modelled with a **skip-gram** model. A skip-gram collects the co-occurence statistics of a word with the words that surround it within some fixed **context window**, to the left of the word, and to the right of the word. \n",
    "\n",
    "Consider the following fragment of a sentence in which a context window of 3 words around the word _fox_ are shown:\n",
    "\n",
    "> ... the quick brown **fox** jumped over the ...\n",
    "\n",
    "The skipgram counts that we then extract from this fragment are \n",
    "\n",
    "$$C(quick, fox) = C(brown, fox)= C(jumped, fox) = C(over, fox) = 1,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$C(the, fox) = 2.$$\n",
    "\n",
    "After all theses counts have been collected, we normalize and divide, giving probabilities $p(quick, fox), p(brown, fox)$ etc. such that\n",
    "\n",
    "$$\\sum_{\\{w,v\\}} p(w,v) = 1.$$\n",
    "\n",
    "**[Note]** The word-pairs $\\{w,v\\}$ are **unordered**. So $p(quick, fox) = p(fox, quick)$.\n",
    "\n",
    "----\n",
    "\n",
    "We have provided you below with all the code nessecary to obtain these distributions. Read this through, and pay some attention to the methods that they are provided with. These will be usefull in some of the questions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "As always, we will obtain the distributions from a large text corpus. To read in such a text we provide you with the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    \"\"\"\n",
    "    Stores text data with additional attributes.\n",
    "    :param fname: a path to a txt file\n",
    "    \"\"\"\n",
    "    def __init__(self, fname):\n",
    "        self._fname = fname\n",
    "        self._data = []\n",
    "        self._w2i = defaultdict(lambda: len(self._w2i))\n",
    "        self._i2w = dict()\n",
    "        self._counter = Counter()\n",
    "        self._ntokens = 0    # number of tokens in dataset\n",
    "        \n",
    "        self._read()\n",
    "        \n",
    "    def _read(self):\n",
    "        with open(self._fname, \"r\") as fh:\n",
    "            for line in fh:\n",
    "                tokens = line.strip().lower().split()\n",
    "                self._data.append(tokens)\n",
    "                self._counter.update(tokens)\n",
    "        \n",
    "        # Store number of tokens in the text\n",
    "        self._ntokens = sum(self._counter.values())\n",
    "        \n",
    "        # Store the words in w2i in order of frequency from high to low\n",
    "        for word, _ in self._counter.most_common(self._ntokens):\n",
    "            self._i2w[self._w2i[word]] = word\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of tokens in the dataset\n",
    "        \"\"\"\n",
    "        return self._ntokens\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        The data as list of lists\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def counter(self):\n",
    "        \"\"\"\n",
    "        The word-counts as counter\n",
    "        \"\"\"\n",
    "        return self._counter\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "        Number of words in the dataset\n",
    "        \"\"\"\n",
    "        return len(self._w2i)\n",
    "    \n",
    "    @property\n",
    "    def w2i(self):\n",
    "        \"\"\"\n",
    "        Word to index dictionary\n",
    "        Words are sorted in order of frequency from high to low\n",
    "        \"\"\"\n",
    "        return self._w2i\n",
    "    \n",
    "    @property\n",
    "    def i2w(self):\n",
    "        \"\"\"\n",
    "        Inverse dictionary of w2i: index to words\n",
    "        \"\"\"\n",
    "        return self._i2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram and Skipgram\n",
    "\n",
    "To train unigram and skipgram language models, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram:\n",
    "    \"\"\"\n",
    "    A unigram language model\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "        self._unigram_counts = defaultdict(int)\n",
    "        self._unigram_probs = defaultdict(float)\n",
    "        self._unigram_counter = Counter()\n",
    "        self._unigram_distribution = Counter()\n",
    "        self._train()\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Trains the model trained on data\n",
    "        \"\"\"\n",
    "        # Get the word counts\n",
    "        for sent in self._data:\n",
    "            for word in sent:\n",
    "                self._unigram_counts[word] += 1\n",
    "            \n",
    "        # Normalize the word counts\n",
    "        s = sum(self._unigram_counts.values())\n",
    "        for word_pair, count in self._unigram_counts.items():\n",
    "            self._unigram_probs[word_pair] = count / s\n",
    "        \n",
    "        # There are some advantages to additionally use a Counter\n",
    "        self._unigram_counter = Counter(self._unigram_counts)\n",
    "        self._unigram_distribution = Counter(self._unigram_probs)\n",
    "        \n",
    "    def prob(self, w):\n",
    "        \"\"\"\n",
    "        Returns the unigram probability p(w)\n",
    "        \"\"\"\n",
    "        return self._unigram_probs[w]\n",
    "    \n",
    "        \n",
    "    def count(self, w):\n",
    "        \"\"\"\n",
    "        Returns the unigram count c(w)\n",
    "        \"\"\"\n",
    "        return self._unigram_counts[w]\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        Returns the data\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def counter(self):\n",
    "        \"\"\"\n",
    "        Returns the unigram counts as Counter\n",
    "        \"\"\"\n",
    "        return self._unigram_counter\n",
    "    \n",
    "    @property\n",
    "    def distribution(self):\n",
    "        \"\"\"\n",
    "        Returns the unigram distribution as Counter\n",
    "        \"\"\"\n",
    "        return self._unigram_distribution\n",
    "    \n",
    "    \n",
    "class Skipgram:\n",
    "    \"\"\"\n",
    "    A skip-gram language model\n",
    "    Note: p(w,v) = p(v,w)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, context_window=5):\n",
    "        self._data = data\n",
    "        self._context_window = context_window\n",
    "        self._skipgram_counts = defaultdict(int)\n",
    "        self._skipgram_probs = defaultdict(float)\n",
    "        self._skipgram_counter = Counter()\n",
    "        self._skipgram_distribution = Counter()\n",
    "        self._train()\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Trains the model\n",
    "        \"\"\"\n",
    "        # Get the co-occurrence counts\n",
    "        for sent in self._data:\n",
    "            for i in range(self._context_window, len(sent) - self._context_window):\n",
    "                context = [sent[i - j] for j in range(1, self._context_window + 1)] + \\\n",
    "                          [sent[i + j] for j in range(1, self._context_window + 1)]    \n",
    "                w = sent[i]\n",
    "                for v in context:\n",
    "                    word_pair = tuple(sorted([w, v])) # causes p(w,v) = p(v,w)\n",
    "                    self._skipgram_counts[word_pair] += 1\n",
    "            \n",
    "        # Turn the co-occurrence counts into probabilities\n",
    "        s = sum(self._skipgram_counts.values())\n",
    "        for word_pair, count in self._skipgram_counts.items():\n",
    "            self._skipgram_probs[word_pair] = count / s\n",
    "        \n",
    "        # There are some advantages to additionally use a Counter\n",
    "        self._skipgram_counter = Counter(self._skipgram_counts)\n",
    "        self._skipgram_distribution = Counter(self._skipgram_probs)\n",
    "        \n",
    "    def prob(self, w, v):\n",
    "        \"\"\"\n",
    "        Returns the skip-gram probability\n",
    "        Note: p(w,v) = p(v,w)\n",
    "        \"\"\"\n",
    "        word_pair = tuple(sorted([w,v]))\n",
    "        \n",
    "        return self._skipgram_probs[word_pair]\n",
    "    \n",
    "    def count(self, w, v):\n",
    "        \"\"\"\n",
    "        Returns the skip-gram counts count(w,v) = count(v,w)\n",
    "        \"\"\"\n",
    "        word_pair = tuple(sorted([w,v]))\n",
    "        \n",
    "        return self._skipgram_counts[word_pair]\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        Returns the data\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def context_window(self):\n",
    "        \"\"\"\n",
    "        Returns the context window size\n",
    "        \"\"\"\n",
    "        return self._context_window\n",
    "    \n",
    "    @property\n",
    "    def counter(self):\n",
    "        \"\"\"\n",
    "        Returns the skipgram counte as Counter,\n",
    "        so that we can use the method most_common.\n",
    "        \"\"\"\n",
    "        return self._skipgram_counter\n",
    "    \n",
    "    @property\n",
    "    def distribution(self):\n",
    "        \"\"\"\n",
    "        Returns the skipgram probs as Counter,\n",
    "        so that we can use the method most_common.\n",
    "        \"\"\"\n",
    "        return self._skipgram_distribution\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text corpora\n",
    "\n",
    "We have the following text corpora availlable for you:\n",
    "* A fragment of the _Penn Treebank_ with around 900.000 tokens\n",
    "* A collection of _TedX_ talks with around 5 million tokens\n",
    "* A collection of _Wikipedia_ entries, contained around 83 million tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Penn Treebank*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 583 ms, sys: 37 ms, total: 620 ms\n",
      "Wall time: 631 ms\n",
      "Number of tokens:  887521\n",
      "Vocabulary size:  9999\n"
     ]
    }
   ],
   "source": [
    "%time ptb = TextData(\"ptb.txt\")\n",
    "print(\"Number of tokens: \", len(ptb))\n",
    "print(\"Vocabulary size: \", ptb.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.58 s, sys: 153 ms, total: 9.73 s\n",
      "Wall time: 9.87 s\n",
      "CPU times: user 223 ms, sys: 1.08 ms, total: 224 ms\n",
      "Wall time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "%time ptb_skipgram = Skipgram(ptb.data)\n",
    "%time ptb_unigram = Unigram(ptb.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *TedX* talks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 s, sys: 183 ms, total: 3.85 s\n",
      "Wall time: 3.92 s\n",
      "Number of tokens:  4466833\n",
      "Vocabulary size:  9999\n"
     ]
    }
   ],
   "source": [
    "%time ted = TextData(\"ted.txt\")\n",
    "print(\"Number of tokens: \", len(ted))\n",
    "print(\"Vocabulary size: \", ted.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.2 s, sys: 268 ms, total: 38.4 s\n",
      "Wall time: 38.5 s\n",
      "CPU times: user 1.28 s, sys: 7.11 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%time ted_skipgram = Skipgram(ted.data)\n",
    "%time ted_unigram = Unigram(ted.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of *Wikipedia* entries: (This will take some time. Expect around 15 mins for the skipgram model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time wiki = TextData(\"wiki.txt\")\n",
    "# print(\"Number of tokens: \", len(wiki))\n",
    "# print(\"Vocabulary size: \", wiki.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time wiki_skipgram = Skipgram(wiki.data)\n",
    "# %time wiki_unigram = Unigram(wiki.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collocations with PPMI (40 points)\n",
    "\n",
    "$PPMI$ can be used to find [collocations](https://en.wikipedia.org/wiki/Collocation); words that co-occur significantly more often than can be atrributed to chance. $PPMI$ is a natural measure for this task: the collocations are precisely the word-pairs $w_i$ and $w_j$ for which $PPMI(w_i,w_j)$ is high!\n",
    "\n",
    "Have a look at the bottom of the [PMI wikipedia entry](https://en.wikipedia.org/wiki/Pointwise_mutual_information): there it lists a number of such collocations found with $PMI$.\n",
    "\n",
    "You will try to find collocations in the text-data that we've provided you with. First, you will need a function that computes the $PPMI$ given a $p(x)$, $p(y)$ and $p(x,y)$.\n",
    "\n",
    "**(a)** Complete the function `PPMI` below. **(20 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def PPMI(unigram, skipgram, x, y):\n",
    "    \"\"\"\n",
    "    Returns the positive PMI of x and y\n",
    "    \n",
    "        PPMI(x, y) = max(0, PMI(x, y))\n",
    "        \n",
    "    where PMI(x,y) is computed using the \n",
    "    unigram and skipgram language model\n",
    "        \n",
    "        PMI(x,y) = log [p(x,y) / p(x)p(y)]\n",
    "        \n",
    "    :param unigram: an instance of Unigram\n",
    "    :param skipgram: an instance of Skipgram\n",
    "    :returns ppmi: a float between 0 and +inf\n",
    "    \"\"\"\n",
    "    # in the case the skipgram prob == 0 then we would get a domain error for the log. In this case just return 0.\n",
    "    if skipgram.prob(x,y) == 0.0:\n",
    "        return 0.0 \n",
    "    \n",
    "    pmi = math.log(skipgram.prob(x,y)/(unigram.prob(x) * unigram.prob(y)))\n",
    "    ppmi = max([0, pmi])\n",
    "    return ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB\n",
    "\n",
    "**(a)** Use the function you wrote to find collocations in the *Penn Treebank* dataset. Recall: these are word-pairs in the corpus that have a high $PPMI$. Print a list of at most 30 of word-pairs (and they don't all have to make total sense!).  **(5 points)**\n",
    "\n",
    "**[Hint]** You might find that highest $PPMI$ word-pairs are not really what you are looking for. They make no sense as collocations, for example due to noise arrising from insufficient statistics. In that case you should search a little bit further down the line, with words that have a little lower $PPMI$. Note for example that the collocations in the [PMI wikipedia entry](https://en.wikipedia.org/wiki/Pointwise_mutual_information) have PMI's between 8 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word pair pricings-non-u.s. has high collocation=10.205705661783963\n",
      "the word pair pakistan-isi has high collocation=10.493387734235744\n",
      "the word pair evenly-26-week has high collocation=10.100345146126138\n",
      "the word pair kuala-lumpur has high collocation=10.408229925895437\n",
      "the word pair bran-oat has high collocation=10.562380605722696\n",
      "the word pair jeep-cherokee has high collocation=10.051554981956706\n",
      "the word pair teller-altman has high collocation=10.339237054408486\n",
      "the word pair h.f.-ahmanson has high collocation=10.051554981956706\n",
      "the word pair caffeine-free-pepsi has high collocation=10.051554981956706\n",
      "the word pair alleviate-programmers has high collocation=10.205705661783963\n",
      "the word pair aga-khan has high collocation=10.359856341611222\n",
      "the word pair 13-week-26-week has high collocation=10.388027218577918\n",
      "the word pair ig-metall has high collocation=10.898852842343908\n",
      "the word pair broadcasters-programmers has high collocation=10.205705661783963\n",
      "the word pair sri-basir has high collocation=10.205705661783963\n",
      "the word pair kurt-brouwer has high collocation=10.205705661783963\n",
      "the word pair d'arcy-benton has high collocation=10.205705661783963\n",
      "the word pair isler-skipper has high collocation=10.205705661783963\n",
      "the word pair kan.-wichita has high collocation=10.388027218577918\n",
      "the word pair aer-cluett has high collocation=11.997465131012019\n",
      "the word pair banknote-cluett has high collocation=11.997465131012019\n",
      "the word pair banknote-fromstein has high collocation=11.997465131012019\n",
      "the word pair berlitz-cluett has high collocation=11.997465131012019\n",
      "the word pair berlitz-fromstein has high collocation=11.997465131012019\n",
      "the word pair berlitz-gitano has high collocation=11.997465131012019\n",
      "the word pair calloway-cluett has high collocation=11.997465131012019\n",
      "the word pair calloway-fromstein has high collocation=11.997465131012019\n",
      "the word pair calloway-gitano has high collocation=11.997465131012019\n",
      "the word pair calloway-guterman has high collocation=11.997465131012019\n",
      "the word pair centrust-cluett has high collocation=11.997465131012019\n",
      "the word pair centrust-fromstein has high collocation=11.997465131012019\n",
      "the word pair centrust-gitano has high collocation=11.997465131012019\n",
      "the word pair centrust-guterman has high collocation=11.997465131012019\n",
      "the word pair centrust-hydro-quebec has high collocation=11.997465131012019\n",
      "the word pair cluett-fromstein has high collocation=12.690612311571964\n",
      "the word pair cluett-gitano has high collocation=12.690612311571964\n",
      "the word pair cluett-guterman has high collocation=12.690612311571964\n",
      "the word pair cluett-hydro-quebec has high collocation=12.690612311571964\n",
      "the word pair cluett-ipo has high collocation=12.690612311571964\n",
      "the word pair fromstein-gitano has high collocation=12.690612311571964\n",
      "the word pair fromstein-guterman has high collocation=12.690612311571964\n",
      "the word pair fromstein-hydro-quebec has high collocation=12.690612311571964\n",
      "the word pair fromstein-ipo has high collocation=12.690612311571964\n",
      "the word pair fromstein-kia has high collocation=12.690612311571964\n",
      "the word pair gitano-guterman has high collocation=12.690612311571964\n",
      "the word pair gitano-hydro-quebec has high collocation=12.690612311571964\n",
      "the word pair gitano-ipo has high collocation=12.690612311571964\n",
      "the word pair gitano-kia has high collocation=12.690612311571964\n",
      "the word pair gitano-memotec has high collocation=12.690612311571964\n",
      "the word pair guterman-hydro-quebec has high collocation=12.690612311571964\n",
      "the word pair guterman-ipo has high collocation=12.690612311571964\n",
      "the word pair guterman-kia has high collocation=12.690612311571964\n",
      "the word pair guterman-memotec has high collocation=12.690612311571964\n",
      "the word pair guterman-mlx has high collocation=12.690612311571964\n",
      "the word pair hydro-quebec-ipo has high collocation=12.690612311571964\n",
      "the word pair hydro-quebec-kia has high collocation=12.690612311571964\n",
      "the word pair hydro-quebec-memotec has high collocation=12.690612311571964\n",
      "the word pair hydro-quebec-mlx has high collocation=12.690612311571964\n",
      "the word pair hydro-quebec-nahb has high collocation=12.690612311571964\n",
      "the word pair ipo-kia has high collocation=12.690612311571964\n",
      "the word pair ipo-memotec has high collocation=12.690612311571964\n",
      "the word pair ipo-mlx has high collocation=12.690612311571964\n",
      "the word pair ipo-nahb has high collocation=12.690612311571964\n",
      "the word pair ipo-punts has high collocation=12.690612311571964\n",
      "the word pair kia-memotec has high collocation=12.690612311571964\n",
      "the word pair kia-mlx has high collocation=12.690612311571964\n",
      "the word pair kia-nahb has high collocation=12.690612311571964\n",
      "the word pair kia-punts has high collocation=12.690612311571964\n",
      "the word pair kia-rake has high collocation=12.690612311571964\n",
      "the word pair memotec-mlx has high collocation=12.690612311571964\n",
      "the word pair memotec-nahb has high collocation=12.690612311571964\n",
      "the word pair memotec-punts has high collocation=12.690612311571964\n",
      "the word pair memotec-rake has high collocation=12.690612311571964\n",
      "the word pair memotec-regatta has high collocation=12.690612311571964\n",
      "the word pair mlx-nahb has high collocation=12.690612311571964\n",
      "the word pair mlx-punts has high collocation=12.690612311571964\n",
      "the word pair mlx-rake has high collocation=12.690612311571964\n",
      "the word pair mlx-regatta has high collocation=12.690612311571964\n",
      "the word pair mlx-rubens has high collocation=12.690612311571964\n",
      "the word pair nahb-punts has high collocation=12.690612311571964\n",
      "the word pair nahb-rake has high collocation=12.690612311571964\n",
      "the word pair nahb-regatta has high collocation=12.690612311571964\n",
      "the word pair nahb-rubens has high collocation=12.690612311571964\n",
      "the word pair nahb-sim has high collocation=11.997465131012019\n",
      "the word pair punts-rake has high collocation=12.690612311571964\n",
      "the word pair punts-regatta has high collocation=12.690612311571964\n",
      "the word pair punts-rubens has high collocation=12.690612311571964\n",
      "the word pair punts-sim has high collocation=11.997465131012019\n",
      "the word pair punts-snack-food has high collocation=11.997465131012019\n",
      "the word pair rake-regatta has high collocation=12.690612311571964\n",
      "the word pair rake-rubens has high collocation=12.690612311571964\n",
      "the word pair rake-sim has high collocation=11.997465131012019\n",
      "the word pair rake-snack-food has high collocation=11.997465131012019\n",
      "the word pair rake-ssangyong has high collocation=11.997465131012019\n",
      "the word pair regatta-rubens has high collocation=12.690612311571964\n",
      "the word pair regatta-sim has high collocation=11.997465131012019\n",
      "the word pair regatta-snack-food has high collocation=11.997465131012019\n",
      "the word pair regatta-ssangyong has high collocation=11.997465131012019\n",
      "the word pair regatta-swapo has high collocation=11.997465131012019\n",
      "the word pair rubens-sim has high collocation=11.997465131012019\n",
      "the word pair rubens-snack-food has high collocation=11.997465131012019\n",
      "the word pair rubens-ssangyong has high collocation=11.997465131012019\n",
      "the word pair rubens-swapo has high collocation=11.997465131012019\n",
      "the word pair rubens-wachter has high collocation=11.997465131012019\n"
     ]
    }
   ],
   "source": [
    "# we define a high value\n",
    "high_value = 10\n",
    "max_wordpairs = 30 \n",
    "count_pairs = 0\n",
    "\n",
    "for word_i, index_i in ptb.w2i.items():\n",
    "    if count_pairs > 30:\n",
    "        break\n",
    "    for word_j, index_j in ptb.w2i.items():\n",
    "        # we only check the upper-triangle half of the matrix as the collocation(word_i,word_j)=collocation(word_j,word_i)\n",
    "        if index_j > index_i:\n",
    "            ppmi = PPMI(ptb_unigram, ptb_skipgram, word_i, word_j)\n",
    "            if ppmi > high_value:\n",
    "                print(\"the word pair {}-{} has high collocation={}\".format(word_i, word_j, ppmi))\n",
    "                count_pairs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TedX\n",
    "\n",
    "**(b)** Use the function you wrote to find collocations in the *TedX* dataset. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "\n",
    "**(c)** Find collocations in the *Wikipedia* dataset. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Optional]** Use the following template to query the model for PMI values for word-pairs. In particular, it is interesting to see what PMI our model assigns to the collocations in the PMI wikipedia entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend this list as you like\n",
    "wiki_collocations = [(\"puerto\", \"rico\"), (\"los\", \"angeles\"), (\"hong\", \"kong\"), (\"carbon\", \"dioxide\"),\n",
    "                     (\"star\", \"trek\"), (\"star\", \"wars\"), (\"nobel\", \"prize\"), (\"prize\", \"laureate\"), \n",
    "                     (\"donald\", \"knuth\"), (\"discrete\", \"mathematics\"), (\"the\", \"and\"), (\"of\", \"it\")] \n",
    "\n",
    "print(\"{}{}{}\".format(\"word 1\".ljust(15), \"word 2\".ljust(15), \"PMI\"))\n",
    "print(\"-----------------------------------------------\")\n",
    "for (w, v) in wiki_collocations:\n",
    "    print(\"{}{}{}\".format(w.ljust(15), v.ljust(15), PPMI(wiki_unigram, wiki_skipgram, w, v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Now that you have (hopefully) been succesful at finding some collocations, take a moment to compare the lists of word-pairs from above, for which $PPMI(w_i,w_j)$ is high, with the list of word-pairs for which $p(w_i,w_j)$ is high. **(5 points)**\n",
    "\n",
    "That means: \n",
    "* Print out a list of word-pairs for which $p(w_i,w_j)$ is high (the top 30 will suffice, for the corpus of your choice). \n",
    "* What difference do you see? Is the list of top $p(w_i,w_j)$ word-pairs as useful as the list of top $PPMI(w_i,w_j)$ word-pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word-embeddings via PMI-SVD (60 points)\n",
    "\n",
    "Inspired by [this blog-post](http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/) we consider a classic method to obtain word embeddings that combines $PMI$ with linear algebra.\n",
    "\n",
    "Go ahead and read the post, it's an easy and quick read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI matrix\n",
    "\n",
    "**(a)** The first step is to make a PPMI matrix $P$. This matrix will have entries\n",
    "    \n",
    "$$(P)_{ij} = PPMI(w_i, w_j)$$\n",
    "\n",
    "Where $w_i$ and $w_j$ are the $i$-th and $j$-th words in our `w2i` dictionary. Finish the function `make_ppmi_matrix`. **(20 points)**\n",
    "\n",
    "**[Note]** If you really want to *scale up* (more about that below), you can consider writing a second version of the function that uses a *sparse* matrix datastructe for the $PMI$ matrix. Most of the entries in it will be zero. [Scipy sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) has a number of options. (You will not be able to plot this matrix with `imshow` though, like we will do a in a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppmi_matrix(words, unigram, skipgram):\n",
    "    \"\"\"\n",
    "    Constructs a PPMI matrix of with the words\n",
    "    in words.\n",
    "    :param words: the list of words (not indices!) for which the PPMI\n",
    "                  values will be stored in the matrix\n",
    "    :param unigram: an instance of Unigram\n",
    "    :param skipgram: an instance of Skipgram\n",
    "    :returns P: a numpy array such that P[i,j] = PPMI[words[i], words[j]]\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Use the function you wrote above to construct the $PPMI$ matrix for a corpus of your choice. **(10 points)**\n",
    "\n",
    "**[Note]** You are adviced to start with the relatively small *PTB* dataset, and a small list of words, for example only the top 1000 most frequent words. If you get this working, you can scale up to the *TedX* and *Wikipedia* datasets, and to a list of the top 5,000 or 10,000 words, or even more. (The upper limit depends on your patience, the memory availlable on your laptop, and whether you are using a sparse datastructure). But you should **already expect to get good performance in the 5,000-10,000 words range**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Plot the $PPMI$ matrix with `plt.imshow`. You can use the following template code. What do you notice about the matrix? **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppmi_matrix = ...\n",
    "\n",
    "# The whole PPMI matrix\n",
    "ax, fig = plt.subplots(figsize=(10,10))\n",
    "plt.imshow(ppmi_matrix)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Only the top-left corner (which hold the most frequent words)\n",
    "ax, fig = plt.subplots(figsize=(10,10))\n",
    "plt.imshow(ppmi_matrix[0:300, 0:300])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Now that we have the $PPMI$ co-occurence matrix, we are ready to compute its **singular value decomposition (SVD)**. \n",
    "\n",
    "### Definition\n",
    "\n",
    "SVD is a very elegant linear algebra technique. It is defined as follows. Let $A \\in \\mathbb{R}^{m\\times n}$, then the singular value decomposition of A is given by\n",
    "\n",
    "$$A = U\\Sigma V^{\\top},$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $U$ is an $m \\times m$ **unitary** matrix.\n",
    "    * A real-valued matrix $U$ is called unitary when $U^{\\top}U = UU^{\\top} = I$. In other words: $U$ forms an *orthonormal* basis for $\\mathbb{R}^{m}$.\n",
    "    \n",
    "    \n",
    "* $\\Sigma$ is a **diagonal** $m \\times n$ matrix with **non-negative real numbers**.\n",
    "\n",
    "    * The diagonal values are the so called **singular values** $\\sigma_1, \\sigma_2,\\dots,\\sigma_n$ (supposing $n \\leq m$). The convention is that in the matrix $\\Sigma$ these are ordered from large to small: $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$. \n",
    "    \n",
    "    \n",
    "* $V$ is an $n \\times n$ **unitary** matrix (and $V^{\\top}$ its transpose)\n",
    "\n",
    "\n",
    "### Low-rank approximation\n",
    "\n",
    "When we select only the first $k$ collums of $U$ and $V$ and the first $k$ singular values in $\\Sigma$ we get \n",
    "* $\\tilde{U} \\in \\mathbb{R}^{m \\times k}$\n",
    "* $\\tilde{\\Sigma} \\in \\mathbb{R}^{k \\times k}$\n",
    "* $\\tilde{V} \\in \\mathbb{R}^{n \\times k}.$\n",
    "\n",
    "\n",
    "The reduced matrices can be used to make a rank $k$ matrix $\\tilde{A} \\in \\mathbb{R}^{m\\times n}$. The matrix $\\tilde{A}$ is an approximation of the matrix $A$:\n",
    "\n",
    "$$A \\approx \\tilde{U}\\tilde{\\Sigma} \\tilde{V}^{\\top} = \\tilde{A}.$$\n",
    "\n",
    "Moreover, this approximation is the 'best' approximation of $A$ in the sense that in minimizes the distance to $A$ in a type of matrix norm called the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm):\n",
    "    \n",
    "$$\\tilde{A} = \\arg \\min_{A'} || A - A'||_{F}$$\n",
    "\n",
    "----- \n",
    "\n",
    "You can learn more about SVD from these sources:\n",
    "\n",
    "* [Jurafksy and Martin 16.1](https://web.stanford.edu/~jurafsky/slp3/16.pdf) (3rd edition)\n",
    "* The [Wikipedia page](https://en.wikipedia.org/wiki/Singular-value_decomposition) (with a very good visual illustration)\n",
    "* Or this long but good [blog-post series](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) (if you have some time!)\n",
    "\n",
    "----------\n",
    "\n",
    "For the purposes of this tutorial, however, all we need to know to get our embeddings is how to use SVD in python. And that is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "%time U, s, Vt = svd(wiki_ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the dimension\n",
    "\n",
    "Now we can create the rank $k$ matrix $\\tilde{U}$ by selection only the first $k$ columns of $U$. The value $k$ will be the **embedding dimension** of our word vectors. Now we have our word embeddings! They are the rows of $\\tilde{U}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "U_tilde = U[:,:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the word embeddings we can see that, unlike the full $PMI$ matrix, the vectors in $\\tilde{U}$ are **dense**: almost all values non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(15,15))\n",
    "plt.imshow(U_tilde[:800])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with t-SNE\n",
    "\n",
    "A simple and rewarding way to evaluate to the word embeddings is by visualizing them.\n",
    "\n",
    "When visualizing the vectors, we ideally show them in their $k$-dimensional vector space, but that is impossible to do. To plot them we need to reduce their dimension much beyond $k$.\n",
    "\n",
    "A very popular method for this is [**t-SNE**](https://lvdmaaten.github.io/tsne/). t-SNE is a very effective technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. What it does is find a lower dimensional surface on which to project the high-dimensional data in such a way that the local structure of the original data is preserved as much as possible in the projected data. In simple words: data-points that are close to each other in the original space end up close to each other in the projected space. (Note: this does not hold large distances! Points that are far apart in the original space do not necessarily end up far apart in the projected space.) \n",
    "\n",
    "So, t-SNE is like Principal Component Analysis (PCA), another popular dimensionality reduction technique. But, due to the non-linear nature of the surface it finds (the 'manifold') t-SNE has more flexibility.\n",
    "\n",
    "--------\n",
    "If you want to know more about t-SNE you can read the following sources:\n",
    "* [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/) on Distill discusses, among others, the effects of the *perplexity* parameter in t-SNE\n",
    "* This [Scikit-learn example](http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py)\n",
    "--------\n",
    "\n",
    "The code plots below uses **t-SNE** to make a two-dimensional plot with our word-embeddings. We also throw in some **K-means** clustering *in the original high-dimensional space* so that we can color our dimension-reduced word-embeddings. This gives us some for additional interpretative abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.palettes import d3\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def emb_scatter(data, names, perplexity=30.0, N=20):\n",
    "    \"\"\"\n",
    "    Uses t-SNE with given perplexity to reduce the dimension of the \n",
    "    vectors in data to 2, plots these in a bokeh 2d scatter plot, \n",
    "    and colors them with N colors using K-means clustering of the \n",
    "    originial vectors. The colored dots are tagged with labels from\n",
    "    the list names.\n",
    "    \n",
    "    :param data: numpy array of shape [num_vectors, embedding_dim]\n",
    "    :param names: a list of words of length num_vectors in the same order as data\n",
    "    :param perplexity: the perplexity for t-SNE\n",
    "    :param N: the number of clusters to find by K-means\n",
    "    \"\"\"\n",
    "    ## Try to find some clusters ##\n",
    "    print(\"Finding clusters\")\n",
    "    kmeans = KMeans(n_clusters=N)\n",
    "    kmeans.fit(data)\n",
    "    klabels = kmeans.labels_\n",
    "\n",
    "    ## Get a tsne fit ##\n",
    "    print(\"Fitting tsne\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "    emb_tsne = tsne.fit_transform(data)\n",
    "    \n",
    "    ## Plot the tsne of the embeddings with bokeh ##\n",
    "    # source: https://github.com/oxford-cs-deepnlp-2017/practical-1\n",
    "    p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "               toolbar_location=\"above\",\n",
    "               title=\"T-SNE for most common words\")\n",
    "\n",
    "    # Set colormap as a list\n",
    "    colormap = d3['Category20'][N]\n",
    "    colors = [colormap[i] for i in klabels]\n",
    "\n",
    "    source = ColumnDataSource(data=dict(x1=emb_tsne[:,0],\n",
    "                                        x2=emb_tsne[:,1],\n",
    "                                        names=names,\n",
    "                                        colors=colors))\n",
    "\n",
    "    p.scatter(x=\"x1\", y=\"x2\", size=8, source=source, color='colors')\n",
    "\n",
    "    labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                      text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                      source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Use the function `emb_scatter` to plot the word-vector in $\\tilde{U}$. You are adviced to plot only the first 500-1000 word-vectors to make the resulting plot not too cluttered. **(10 points)**\n",
    "\n",
    "Now go find some interesting clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Give a word cluster that you have found in the scatter plot that you think is particularly nice. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**You have reached the end of this notebook!** You can read on if you are interested.\n",
    "\n",
    "Now, you can note that there are a number of hyperparameters in the model and its visualization that you can experiment with. **If you are motivated, you can experiment with these! But note that you are not required to do this.**\n",
    "\n",
    "The options are:\n",
    "* The pereplixity used in t-SNE has significant effects on the resulting visualization. Generally, values in the range 5-30 work well. If you have less data, or it is very high dimensional, you should go lower. If the data is packed closely together (lower dimension, more data), then higher perplexity generally works better.\n",
    "* The embedding dimension $k$ of the vectors. We chose $k=300$, which is relatively standard in the literature. But you also see dimension 50,100, 150, 500, even 600. Do you notice differences? And why not just even use the original dimensions of $U$? \n",
    "* The number of words you use in the $PPMI$ matrix $P$. How few can you get away with? Is more always better?\n",
    "* Use the matrix product $U\\Sigma$ for the embeddings. The paper by Levy et al. (2015) below argues for this: select the first $k$ columns of $U\\Sigma$ to give $\\tilde{U\\Sigma}$ as embeddings matrix. What this means is that each orthogonal vector $u_i$ is scaled by its singular value $\\sigma_i$ to give $\\sigma_i u_i$ before to reducing it to $\\tilde{\\sigma_i u_i}$.\n",
    "\n",
    "How to evaluate the quality of word-embeddings quantitatively? Read the following for more information on the various evaluation methods developped for this:\n",
    "* [Lecture notes](http://cs224d.stanford.edu/lecture_notes/notes2.pdf) from the Stanford course [Deep Learning for NLP](http://web.stanford.edu/class/cs224n/) \n",
    "\n",
    "The following paper compares a number the most popular word-embedding techniques with each. PPMI-SVD is one of them! Read the paper to get inspiration for your experiments to improve the above 'vanilla' implementation: \n",
    "\n",
    "* [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016) (Levy et al. 2015)\n",
    "\n",
    "The following blog-post contains an interesting discussion on why low dimensional embeddings often work better than very high-dimensional ones.\n",
    "* [Word Embeddings: Explaining their properties](http://www.offconvex.org/2016/02/14/word-embeddings-2/) \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
